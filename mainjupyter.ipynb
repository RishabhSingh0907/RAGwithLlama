{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5090,"status":"ok","timestamp":1731550647817,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"GrjxAn27nqlQ","outputId":"13b6cabb-00c3-401f-c388-5b851ba67c01"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive') # Mount to the standard location"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_GLZoBOAq4gM","executionInfo":{"status":"ok","timestamp":1731550647818,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["# Now you can access your project directory:\n","\n","import os\n","project_path = '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama'\n","os.chdir(project_path)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"SzLfv8R3UWkL","executionInfo":{"status":"ok","timestamp":1731550656260,"user_tz":-330,"elapsed":8449,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["# !pip3 install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n","!pip install -q llama-index-embeddings-huggingface\n","!pip install -q llama-index peft auto-gptq optimum bitsandbytes transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5310,"status":"ok","timestamp":1731550661565,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"apXYXqUxUWkO","outputId":"cdc5279b-3838-4ef0-b4d8-b6ea88906e43"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["import os\n","import torch\n","device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"dvfGIY-3UWkP","executionInfo":{"status":"ok","timestamp":1731550663796,"user_tz":-330,"elapsed":2238,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["from google.colab import userdata\n","HF_TOKEN = userdata.get(\"HF_TOKEN\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Z9acrr--UWkQ","executionInfo":{"status":"ok","timestamp":1731550672401,"user_tz":-330,"elapsed":8607,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n","from llama_index.core.retrievers import VectorIndexRetriever\n","from llama_index.core.query_engine import RetrieverQueryEngine\n","from llama_index.core.postprocessor import SimilarityPostprocessor\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding"]},{"cell_type":"markdown","metadata":{"id":"wie6VMjiUWkQ"},"source":["#### Setting up knowledge base"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7237,"status":"ok","timestamp":1731550679632,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"mhagjpy7UWkS","outputId":"30d59354-c190-48e5-af79-9085a0cbc640"},"outputs":[{"output_type":"stream","name":"stdout","text":["LLM is explicitly disabled. Using MockLLM.\n"]}],"source":["Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n","Settings.llm = None\n","Settings.chunk_size = 128\n","Settings.chunk_overlap = 15"]},{"cell_type":"markdown","metadata":{"id":"xW0LSO6rq8QN"},"source":["#### Fetching Document"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nbn3TzjAUWkS","executionInfo":{"status":"ok","timestamp":1731550679633,"user_tz":-330,"elapsed":68,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["documents = SimpleDirectoryReader(\"Articles\").load_data()"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":67,"status":"ok","timestamp":1731550679633,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"5QDrdWprUWkT","outputId":"b53954bf-125e-404b-b24b-2c94adfa8813"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n"]},{"output_type":"execute_result","data":{"text/plain":["[Document(id_='ae3bd3fa-d774-45b8-852d-415f50964338', embedding=None, metadata={'page_label': '1', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='YOLO-Z: Improving small object detection in YOLOv5 for\\nautonomous vehicles\\nAduen Benjumea* Izzeddin Teeti† Fabio Cuzzolin† Andrew Bradley*\\n17065125@brookes.ac.uk, 19136994@brookes.ac.uk,\\nfabio.cuzzolin@brookes.ac.uk, abradley@brookes.ac.uk\\nAbstract\\nAs autonomous vehicles and autonomous racing rise in popularity, so does the need for faster and more accurate\\ndetectors. While our naked eyes are able to extract contextual information almost instantly, even from far away,\\nimage resolution and computational resources limitations make detecting smaller objects (that is, objects that occupy\\na small pixel area in the input image) a genuinely challenging task for machines and a wide-open research ﬁeld. This\\nstudy explores how the popular YOLOv5 object detector can be modiﬁed to improve its performance in detecting\\nsmaller objects, with a particular application in autonomous racing. To achieve this, we investigate how replacing\\ncertain structural elements of the model (as well as their connections and other parameters) can affect performance\\nand inference time. In doing so, we propose a series of models at different scales, which we name ‘YOLO-Z’, and\\nwhich display an improvement of up to 6.9% in mAP when detecting smaller objects at 50% IOU, at the cost of\\njust a 3ms increase in inference time compared to the original YOLOv5. Our objective is to inform future research\\non the potential of adjusting a popular detector such as YOLOv5 to address speciﬁc tasks and provide insights on\\nhow speciﬁc changes can impact small object detection. Such ﬁndings, applied to the broader context of autonomous\\nvehicles, could increase the amount of contextual information available to such systems.\\n1 Introduction\\nDetecting small objects in images can be challenging, mainly due to limited resolution and context information avail-\\nable to a model [2]. Many modern systems that implement object detection do so at real-time speeds, setting speciﬁc\\nrequirements in computational resources, especially if the processing is to happen on the same device that captures\\nthe images. This is the case for many autonomous vehicle systems [4], where the vehicle itself captures and processes\\nimages in real-time, often to inform its next actions. In this context, detecting smaller objects means detecting objects\\nfarther away from the car, thus allowing earlier detection of such objects, effectively expanding the detection range of\\nthe vehicle. Improvements in this speciﬁc area would better inform the system, allowing it to make more robust and\\nviable decisions.\\nDue to the nature of object detectors, the details of smaller objects lose signiﬁcance as they are processed by each\\nlayer of their convolutional backbone. In this study, by ‘small objects’, we refer to objects which occupy a small pixel\\narea in the input image.\\nEfforts have been made to improve the detection of smaller objects [19], but many revolve around directing the pro-\\ncessing around a speciﬁc area of the image [29, 28, 27] or are focused around two-stage detectors, which are known for\\nachieving better performance at the cost of inference time, making them less suited for real-time applications. This is\\nalso the reason why so many single-stage detectors have been developed for this type of applications [31]. Increasing\\nthe input image resolution is another obvious way to bypass this issue which results, however, in a signiﬁcant increase\\nin processing time.\\nYOLOv5 is a very popular single-stage object detector [11] known for its performance and speed with a clear and\\nﬂexible structure that can be broken down, adjusted and built on a very widely accessible platform. Many of the\\nsystems that apply this architecture and attempt to optimise it, however, they mainly rely on adjusting speciﬁc param-\\neters or augmenting their training set to improve performance [33], without much consideration for structural changes\\n*Visual Artiﬁcial Intelligence Laboratory, Oxford Brookes University, UK\\n†Autonomous Driving & Intelligent Transport Group, Oxford Brookes University, UK\\n1\\narXiv:2112.11798v4  [cs.CV]  3 Jan 2023', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='de3890d0-6253-4bdf-b5bb-be1fc371be10', embedding=None, metadata={'page_label': '2', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='to the model itself to better adapt it for a speciﬁc use case. While YOLOv5 is a potent tool, it is designed to be a\\ngeneral-purpose object detector and therefore is not optimised to detect smaller objects.\\nThis study proposes ways in which YOLOv5 can be modiﬁed to better perform on a given system in terms of small\\nobject detection, with clear real-world implications [4]. We consider, in particular, the case of an autonomous racing\\nvehicle that needs to detect differently coloured cones to drive around a track. We will discuss the effects of differ-\\nent techniques and propose modiﬁed models able to perform this task better while maintaining real-time processing\\nspeeds. The contributions of this paper are:\\n1. A modiﬁed model of YOLOv5 speciﬁcally designed for better detections of small objects.\\n2. Proposing a methodology to modify the structure of YOLOv5 to improve performance in a particular task. This\\nis done in an experimental manner, analysing the different elements that make YOLOv5.\\n2 Related work\\nThis study aims at reﬁning the already existing YOLOv5 model to deal with the many problems associated with\\nsmall object detection. This task is a complex area of machine learning that very quickly escalates in complexity as\\nrequirements evolve. To work with such systems, it is essential to understand the bases upon which they are built, the\\nmany different technologies and techniques that form the current state of the art and the related use cases.\\n2.1 One-stage vs two-stage objects detectors\\nWe know we can classify object detectors into two categories: one-stage and two-stage detectors [19]. The latter\\ntypically decomposes the detection task into (i) region proposal generation and (ii) classiﬁcation, as is the case with\\nFaster R-CNN and its predecessors [6, 5, 26]. While there have been efforts to improve the small object detection\\nability of such models [3], a lot of the attention has been put on performance regardless of inference time. Two stage\\ndetectors have however improved signiﬁcantly over time by streamlining their structure and data ﬂow.\\n2.2 The YOLO family\\nAs a family of object detectors, YOLO takes this idea a step further and has grown very popular over the last few years.\\nWith YOLOv1 [23], object detection is presented as a regression task, thus simplifying the networks and allowing us\\nto build faster models that can be used in real-time. Later versions of YOLO improve different aspects of the model\\n[23, 24, 25, 1]. Most notably, much effort has been spent on the backbone through the different versions. This begs\\nthe question: What potential is there still untapped if changes to an isolated element can have such an impact?\\nYOLOv5 [11] was released very shortly after YOLOv4 [1]. Despite its name, the authors are not directly related,\\nand there have been discussions on whether it is fair to call YOLOv5 a successor of YOLOv4. This implementation\\nprovides similar performance to YOLOv4 and shares the same design. The main point of attention is the fact that it\\nis fully written in the PyTorch framework [20] as opposed to using any form of the Darknet framework [22] and has\\na focus on accessibility and use in a wider range of development environments. Additionally, the models in YOLOv5\\nprove to be signiﬁcantly smaller, faster to train and more accessible to be used in a real-world application.\\n2.3 Systems using and modifying YOLOv5\\nYOLO has been used in many applications requiring the detection of objects. In safety helmet detection systems\\n[34], for instance, YOLO can be adjusted and implemented in series with the rest of a system. Similarly, face masks\\ndetectors have been seen at the entrances of metro stations [33]. Both of these applications do a good job at exploiting\\nthe beneﬁts of YOLO for the detection of smaller objects [21], but do not go as far as modifying the architecture.\\nOther systems that do make an effort to optimize YOLOv5 do so in a limited fashion. Once again, mask detectors [16]\\nhave been proposed that leverage anchors generated and data augmentation to ﬁt a model to the use case better. More', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='722caa21-97dc-4ea6-b6ce-ec1c02ec7402', embedding=None, metadata={'page_label': '3', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='complex systems for helmet detection [10] also do a great job at leveraging the contextual information around small\\nobjects to isolate them and facilitate their detection. However, their approach is not quite universally applicable and\\ncomes at the cost of introducing a two-step process.\\nTypical adjustments to the internal structures of the model are surface-level. In a recent apple detection system [32],\\nthe backbone of YOLOv5 is slightly modiﬁed to simplify it, which offers the potential to adapt to the system’s re-\\nquirements and one that opens the way for additional changes. If a single backbone element is modiﬁed, more drastic\\nchanges can be applied for additional effects.\\n2.4 Small object detection\\nSome effort has been put into developing systems which direct the processing towards certain areas of the input image\\n[29, 28, 27], which allows us to adjust resolution and therefore bypass the limitation of having fewer pixels deﬁning an\\nobject. This approach, however, is better suited for systems that are not time-sensitive, as they require multiple passes\\nthrough a network at different scales. This idea of paying more attention to speciﬁc scales can nevertheless inspire the\\nway we treat certain feature maps.\\nAdditionally, a lot can be learned by looking at how feature maps can be treated instead of just modifying the backbone.\\nDifferent types of feature pyramid networks (FPN) [13, 30, 15] can aggregate feature maps differently to enhance a\\nbackbone in different ways. Such techniques prove to be rather effective.\\n2.5 Autonomous vehicles\\nWithin autonomous driving, object detection can provide valuable contextual information about the vehicle’s surround-\\nings and heavily inform its decision making process [17, 4]. In this case, smaller objects translate to objects further\\naway, meaning a more complete context for the system to make use of. These systems heavily focus on inference time,\\nsacriﬁcing performance if needed, but work can be done to improve them at minimal cost. Performance in this ﬁeld is\\ncritical, as a small improvement in this system can greatly impact the entire vehicle. A common requirement in this\\narea is for detectors to be single-stage [31], for the simple reason that fewer steps and transitions between them often\\ntranslates into fewer resources needed.\\n3 Methodology\\nYOLOv5 provides four different scales for their model, S, M, L and X which stand for Small, Medium, Large, and\\nXlarge, respectively. Each of these scales applies a different multiplier to the depth and width of the model, meaning\\nthe overall structure of the model remains constant, but the size and complexity of each model are scaled. In Our\\nexperiments, we apply changes to the structure of the models individually across all the scales and treat each one as a\\ndifferent model for the purposes of evaluating their effect.\\nTo set a baseline, we trained and tested the unmodiﬁed versions of the four scales of YOLOv5. We then tested changes\\nto these networks individually in order to observe their impact separately against our baseline results. The techniques\\nand structures that did not appear to contribute to better accuracy or inference time were ﬁltered out when moving to the\\nnext phase. We then attempted combinations of the selected techniques. This process was repeated, observing whether\\ncertain techniques complemented or diminished each other and adding more complex combinations progressively.\\nWe ﬁrst discuss the appropriate evaluation metric for our work (Section 3.1), and the dataset used for our investigation\\n(Section 3.2). We then move on to describe our plans to apply a number of model changes to be run under controlled\\ncircumstances (Section 3.2), logging and adjusting as we move through different stages.\\n3.1 Evaluation metric\\nThe original implementation of YOLOv5 provides compatibility with Microsoft Common Objects in Context (COCO)\\nAPI’s [14] metrics at three different object scales (bounding box areas) and Intersection over Unions (IOU ), which\\nproves useful for the purpose of this study. The way values at speciﬁc scales are calculated can give us a good indication', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='6a7504e5-731c-47e4-9b2e-247308b59d02', embedding=None, metadata={'page_label': '4', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='of the performance of the model, but may be slightly inaccurate in extreme cases, which will not be a problem for the\\nmost part, but must be kept in mind.\\nSince these metrics are only compatible with the COCO dataset by default, we have re-implemented this method in\\nour testing code in order to obtain more valuable ﬁgures for our study while using any dataset. Our metric module\\nwill calculate values for large, medium and small objects, in addition to the overall performance. The categorisation\\nof objects into these three categories depends on the following thresholds: ‘small’, if the object occupy an area less\\nthan 32 squared pixels, ‘large’, if the area is more than 96 squared pixels, and ‘medium’, for anything between the two\\nthresholds. In other words, small < 322 < medium < 962 < large.\\n3.2 Dataset and Experimental setup\\nFigure 1: Dataset class instances\\nTo train our models and inform our experiments we adopted\\na dataset of annotated cones from the perspective of an au-\\ntonomous racing car. Its original purpose is to help plan a path\\nfor an autonomous racing vehicle based on the colours of the\\ncones, knowing that there are a total of 4 classes (yellow, blue,\\norange and big orange cones) and close to 4,000 images (see\\nFigure 1, 2). This dataset includes digitally augmented images\\n[18] and cases with challenging weather conditions. A dataset\\nsuch as this one models more complex tasks in autonomous\\nvehicles. Cones are themselves objects that we would ﬁnd on\\nthe road and share many qualities with other objects such as of\\ntrafﬁc signs in terms of size and position.\\nAlthough the dataset would beneﬁt from a larger size, it is\\ncharacterised by a very high object density, with over 30,000\\nlabelled objects. Furthermore, looking at Figure 1, we can ob-\\nserve a very high bias towards the blue and yellow classes.\\nThis makes sense as they serve to mark the two sides of the\\nracing track, but it does constitute an imbalance that will af-\\nfect the overall results (see Section 4). The performance on\\nthese classes will be taken into account when evaluating the\\nmodels, namely by averaging the scores of the most prominent\\nclasses.\\nCones are naturally small objects already in comparison to\\nother objects commonly found in the autonomous driving scenario, such as other vehicles or pedestrians. The correl-\\nogram (a chart of correlation statistics) in Figure 3 shows the position, width, and height of the bounding boxes of the\\nobjects (cones) in the dataset. Our dataset features a high concentration of smaller object boxes, slightly elongated as\\nto be expected because of perspective projection. This high proportion of small objects makes it beneﬁcial for this type\\nof study, as it largely overcomes the issue with a lack of such objects in other popular datasets including MS COCO\\n[12].\\nFigure 2: Sample image from dataset\\nThe dataset was split into training, validation and testing with\\na ratio of 65:15:20. The validation set then informs the training\\nof the model, but is not as relevant as the other two, hence the\\nlower size.\\nThe training for all the experiments was executed in an envi-\\nronment that has 4 Nvidia GTX 1080 GPUs, each has 12 GB\\nVRAM. For testing, however, we used a single GTX1080TI\\nGPU with a batch size of 1, and an i7-6900K CPU working at\\n3.20GHz.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='596d6394-9032-4918-bd58-da85c13a6e32', embedding=None, metadata={'page_label': '5', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3 Proposed architectural changes\\nFigure 3: Relation between the position (in x and y value of the center\\npoint), width and height of instances of the dataset\\nFigure 4: YOLOv5 default structure. In the text we refer to\\nthe elements of this architecture modiﬁed in our work.\\nYOLOv5 uses a yaml ﬁle to instruct a parser how to\\nbuild a model. We use this setup to write our own high-\\nlevel instructions on how different building blocks of the\\nmodel are built and with what parameters, hence modify-\\ning its structure. to implement new structures we arrange\\nand give parameters to each building block or layer and\\ninstruct the parser on how to build it if necessary. In our\\nwords, we make use of the base and experimental net-\\nwork blocks provided with YOLOv5, while implement-\\ning additional blocks where needed to simulate the re-\\nquired structures.\\n3.3.1 Backbone\\nThe backbone of a model is the element dedicated to tak-\\ning the input image and extracting feature maps from it.\\nThis is a crucial step in any object detector, as it is the\\nmain structure responsible for extracting contextual in-\\nformation from the input image as well as for abstracting\\nthat information into patterns. We experimented with re-\\nplacing the existing backbone in YOLOv5 with two sep-\\narate options. ResNet [8] is a popular structure that intro-\\nduces residual connections to lessen the effect of the di-\\nminishing return we observe in deeper neural networks.\\nDenseNet [9] uses similar connections to preserve as\\nmuch information as possible as it moves through the\\nnetwork. Implementing these structures requires break-\\ning them down to their fundamental blocks and ensuring\\nthe layers communicate appropriately. This includes en-\\nsuring the right feature map dimensions, which at times\\nrequires slightly modifying the scaling factor for the\\nwidth and depth of the model.\\nIn both cases, it was important to avoid drastically devi-\\nating the number of layers from the original as to main-\\ntain a comparable complexity. Hence, ResNet50 was\\nused and we downscaled DenseNet proportionally so it reattains its core functionality.\\nAdditionally, YOLOv5 makes use of a Spatial Pyramid Pooling (SPP) [7] layer in between the backbone and the neck.\\nIn our work, however, we have maintained this layer untouched.\\n3.3.2 Neck\\nWe term ‘neck’ the structure placed between the head and backbone (see Figure 4) whose objective is to aggregate as\\nmuch information extracted by the backbone as possible before it is fed to the head. This structure plays a major role\\nin transferring small-object information by preventing it from being lost to higher levels of abstraction. It does this by\\nupsampling the resolution of the feature maps once again so different layers from the backbone can be aggregated and\\nregain inﬂuence on the detection step. [15].\\nIn this work, we simpliﬁed the current Pan-Net [15] to that of an FPN [13] and replaced it a with biFPN [30]. In\\nboth cases the neck retains a similar functionality, but varies in complexity and therefore the number of layers and\\nconnections required for their implementation.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='1927cbda-fccf-4fdf-afb2-71588fca5e97', embedding=None, metadata={'page_label': '6', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.3.3 Other modiﬁcations\\nFigure 5: Example of how favoring smaller feature maps can be implemented\\nin terms of structure both inclusively and exclusively.\\nThe head of the model is responsible for taking\\nfeature maps and inferring the bounding boxes\\nand classes by taking in several aggregated feature\\nmaps from the neck. This structure can remain un-\\ntouched, other than the parameters it receives, as\\nit is a fundamental part of the model that does not\\nhave as much impact in small object detection as\\nthe aforementioned elements.\\nThere are, however, other elements that can have\\nan impact on small object detection performance.\\nOther than input image size, the depth and width\\nof the model can be modiﬁed in order to change\\nwhat aspect of the network the bulk of the process-\\ning goes towards. The way layers are connected\\ncan also be manually altered in the neck and head\\nin order to focus on detecting certain feature maps.\\nIn this study we explored the effect of redirecting\\nthe connections involving higher-resolution fea-\\nture maps, in order for them to be fed directly to\\nthe neck and head. This can be done in an ‘inclu-\\nsive’ manner by expanding the neck to ﬁt an extra\\nfeature map, or in an ‘exclusive’ fashion by replac-\\ning the lowest-resolution feature map in order to ﬁt\\nthe new one, Figure 5 shows both options, as well\\nas the default (original) layout. Making use of a\\nhigher resolution feature map would usually im-\\nprove performance on smaller object at the cost of\\ninference time and potentially detection of larger\\nobjects, similar to the effect of increasing input im-\\nage size. We integrate this behaviour in the neck in\\nthese two ways to minimize the downsides while\\nmaking the most out of its beneﬁts.\\nNote that a number of parameters will have to be adjusted to the new structure, as the learning capabilities of the\\nnetwork can be affected. Mainly, the sizes of the anchor boxes applied in the head, which need to adjust to the\\nresolution of the feature maps being used.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='2266d41c-962e-49c1-8db4-3183bca35641', embedding=None, metadata={'page_label': '7', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 6: Results of applying individual architectural changes to YOLOv5 at each scale. We report the mAp at 50% IOU across all objects\\nsizes (top), the mAP at 50% IOU for small objects only (middle), and the inference speed in frames per second (fps, bottom). YOLOv5: the\\nbaseline. lr02: changing the learning rate to 0.02. lr005: changing the learning rate to 0.005. resnet50: changing the backbone to that of ResNet50.\\ndensenet: changing the backbone to that of DenseNet. 3anch: auto-generating 3 anchors per scale. 5anch: auto-generating 5 anchors per scale.\\nfpn: changing neck to that of FPN. bifpn: changing neck to that of BiFPN. deep: increasing the depth modiﬁer to that of the next scale up (or\\nequivalent). wide: increasing the width modiﬁer to that of the next scale up (or equivalent). XSinc: setting an extra small feature map inclusively\\n(see 3.3.3). XSex: setting an extra small feature map exclusively (see 3.3.3).\\n4 Results\\nModel Features\\nYOLO-Z S DenseNet, FPN, 3 anchors, extra small exclusive feature map\\nYOLO-Z M DenseNet, FPN, 5 anchors, extra small exclusive feature map\\nYOLO-Z L DenseNet, FPN, 5 anchors, extra small exclusive feature map\\nYOLO-Z X DenseNet, bi-FPN, 5 anchors, extra small exclusive feature map\\nTable 1: Modiﬁcations applied to YOLOv5 to achieve models of the YOLO-Z family. Each scale uses its YOLOv5 equivalent as a base.\\nmAP .5 mAP .5 small inference (ms)\\nScales YOLOv5 YOLO-Z difference YOLOv5 YOLO-Z difference YOLOv5 YOLO-Z difference\\nS 0.926 0.955 3.13% 0.869 0.925 6.44% 8 8.9 0.9\\nM 0.932 0.9605 3.06% 0.8795 0.9425 7.16% 11.6 14.3 2.7\\nL 0.935 0.964 3.10% 0.886 0.9545 7.73% 16.6 19.6 3\\nX 0.9385 0.9605 2.34% 0.8975 0.9465 5.46% 26.9 30.6 3.7\\nTable 2: Comparing performance and inference time of YOLOv5 and YOLO-Z (optimal values for each scale in bold).\\nNote that we only show performance on the yellow and blue classes, as they are the best represented in the dataset\\naccording to Figure 1. (See supplementary material ”Individual test results”, Table 1).\\n4.1 Inﬂuence of the backbone\\nA comparison of the performance of the two backbones (see Figure 6) shows that DenseNet consistently exhibits a\\nsigniﬁcant improvement at what appears to be a relatively low ﬁxed increase in inference time (about 3 ms). ResNet\\nnot only seems to worsen performance in most cases, but its inference time is also signiﬁcantly higher, leaving no\\nreason to consider it further at this stage. Our conclusion is that DenseNet is therefore a better ﬁt, in general, for small\\nscale object detection. In the smaller scale models, this can be due to not having networks deep enough to reap the\\nbeneﬁts of a ResNet backbone, while DenseNet does a good job at preserving feature maps’ details.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='9c357bca-45a9-4c75-b0b8-a470b65e9f6e', embedding=None, metadata={'page_label': '8', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Figure 7: Performance comparison between the YOLOv5 and YOLO-Z families of models, plotting mAP (top), mAP for small objects (middle)\\nand mAP for medium objects (bottom) against inference time (ms). Clearly the superior average performance of YOLO-Z is achieved at the smaller\\nscale, while performance is stable and very close to 1 at the medium scale.\\n4.2 Inﬂuence of neck architecture\\nUsing an FPN only outperforms bi-FPN at the S scale (see Figure 6). In the latter case inference time remains fairly\\ncomparable to that of the original YOLOv5 neck, which is not too surprising given their similarities. This might\\nsuggest that simpler models beneﬁt from keeping the feature maps relatively untouched, while other scales require\\nextra steps to adapt to the added processing of the feature maps and eventually outperform the former.\\n4.3 Feature maps\\nIn our experiments, redirecting what feature maps are fed to the neck and head had the most signiﬁcant impact among\\nall techniques. Excluding the lowest resolution feature map in order to replace it with a higher-resolution one ( XS ex\\nin Figure 6) proved particularly effective. This can be attributed to the fact that, after including a higher resolution\\nmap in the head, small objects end up occupying more pixels and having therefore more of an inﬂuence, rather than\\nbeing ‘lost’ in the convolution stages of the backbone. Similarly, getting rid of the original lower-resolution feature\\nmap reduces the amount of processing needed and prevents the model from counteracting the level of detail provided\\nby the higher-resolution map. This is likely a consequence of the dataset used having a very high density of small\\nobjects; performance will likely vary signiﬁcantly in other applications. The only exception to this pattern seems to\\nbe that of the extra large scale ( X), for which the improvement is not as signiﬁcant and keeping the lower resolution\\nfeature maps actually appears detrimental to performance In comparison to the baseline.\\n4.4 Inﬂuence of the number of anchors\\nLetting YOLO generate anchors based on the dataset provided proves effective in performance without affecting\\ninference time. However, The magnitude of the effect and the number of anchors a model favors does seem to be\\naffected by scale (anch3 and anch5 in Figure 6). Again, the dataset used is relevant to this step as, in our tests, most\\ncones will have a similar elongated shape on the y axis (see Figure 3). Other applications with objects varying more\\nin shape might ﬁnd different results.\\nIn terms of scale, the smaller models tend to beneﬁt from fewer anchors, while the opposite is true for the larger scales.\\nNamely, at the S scale having 3 anchors outperforms having 5, while the gap reduces at the M scale. Models L and\\nX, on the other hand, display instead a better performance at 5 anchors. This suggests that more complex or deeper\\nmodels may indeed beneﬁt from additional anchors or, in other words, may be more capable of taking advantage of\\nthe details additional anchors provide.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='8e4e1249-912c-462b-a3e8-12a6383f4e41', embedding=None, metadata={'page_label': '9', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Other factors\\nFigure 8: Visual demonstration of the improved detec-\\ntion results of YOLO-Z S (bottom) compared to YOLOv5\\nS (top) over a region of a sample image covering far away /\\nsmall scale objects. Yellow and blue cone detections are\\nshown as bounding boxes in the respective colours, de-\\ntections missed by both models are shown as red boxes,\\ndetections missed by YOLOv5 but correctly identiﬁed by\\nYOLO-Z S as red circles. One can observe that the im-\\nprovement is most evident for farther away / smaller cones.\\nIn addition to the other major structural changes, a larger learning\\nrate proved to better leverage the models, but this can vary with the\\nnumber of epochs the latter are trained for (lr02 and lr005 in Figure\\n6).\\nInterestingly, a wider model (higher width multiplier) showed to\\nhave a positive effect on the smaller scales as opposed to a deeper\\none ( deep and wide in Figure 6). The opposite is true for the L\\nscale. This might be due to the speciﬁc characteristics of our dataset,\\nin which the objects to be detected form relatively simple patterns\\nwith rather similar features. Nevertheless, more testing would be\\nneeded to determine this, as this pattern is not continued in the extra\\nlarge scale, where both modiﬁcations harm performance by the same\\namount. Additionally, these types of alterations do have a noticeable\\nnegative effect on inference speed, discouraging their use.\\n4.6 Modiﬁed models\\nAdditional tests were carried out using various combinations of the\\naforementioned alteration techniques in order to seek models that\\nfurther deviate from the originals but, at the same time, can further\\nimprove performance. We refer to this proposed family of models at\\ndifferent scales as YOLO-Z, short for ‘YOLO Zoomed’ (see Table\\n1).\\nA comparison of the performance of these new models shows that\\nan FPN neck tends to outperform bi-FPN (compare YOLO-Z X with\\nthe other YOLO-Z models in Table 2) for scales where the opposite\\nwas previously true. Aside from this, we could only observe small\\nvariations across the models. An exception is the X scale, which\\nseems to gain less from such changes and even with the use of a\\ndifferent neck structure does not deliver improvements as signiﬁcant\\nas with the other scales.\\nThis considered, YOLO-Z models achieved an average 2.7 performance increase in absolute mAP at 50% IoU for all\\nobjects and an absolute improvement of 5.9 for small objects at the same IoU across all scales. This comes at the cost\\nof an average 2.6ms increase in inference time.\\n4.7 Discussion\\nIn our investigation of ways in which a popular object detector such as YOLOv5 can be adapted to better detect smaller\\nobjects, we were able identify architectural modiﬁcations delivering a clear improvement in performance compared to\\noriginal at relatively little cost, as the new models retain real-time inference speed.\\nThe context in which we have applied the proposed techniques, that of autonomous racing, is one that can greatly\\nbeneﬁt from such an improvement. As we can see in Figure 8, such changes do have a quantiﬁable impact on detection.\\nIn this work we have not only signiﬁcantly improved the performance of the baseline model, but also identiﬁed a\\nnumber of speciﬁc techniques that can be applied to any other application involving the detection of small or far away\\nobjects.\\nThe net result is that models of the YOLO-Z family outperform those of the YOLOv5 class while retaining an inference\\ntime compatible with a real time application such as autonomous racing (see Table 2 and Figure 7). This is especially\\ntrue for the smaller objects which have been the focus of this study (Figure 7, middle), whereas the performance is\\nstable for medium-sized objects (bottom).', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='4db4de1c-641c-4c85-bb62-b0f4e9135d0f', embedding=None, metadata={'page_label': '10', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Note that, while we have focused here on modifying the popular YOLOv5 model, the methods and techniques we\\nexplored have a potential to be developed into an entirely original model structure.\\nFinally, while this study shows signiﬁcant the empirical gains of the proposed architectural changes, the consistency\\nand generality of the results could and should be further investigated. For instance, the analysis would greatly beneﬁt\\nfrom further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\\nWhile we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\\nunderstood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\\nmore robust solution to small object detection. Additionally, there many more directions and techniques that would ﬁt\\nnicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\\n5 Conclusions\\nIn this study, we have investigated the effects and rationale of different architectural and model alterations applied\\nto the popular YOLOv5 object detector in order to improve its small-object detection abilities. We have validated\\nthese techniques in the autonomous racing scenario, highlighting its speciﬁc needs and limitations, and outlining\\npossible further research. Doing so has produced an original YOLO-Z family of models capable of delivering an\\nimprovement in the ability of detecting small objects (measured by the standard mAP at 50% IoU) close to 6%, while\\nonly increasing inference time by around 3 ms. Using these ﬁndings existing systems can be upgraded to better detect\\nvery small objects in situation in which current models cannot detect anything at all. This can extend an autonomous\\nvehicle’s detection range and perception robustness, leading to better planning and decision making strategies that can\\ngive an autonomous racing car an important edge.\\n6 Acknowledgements\\nThanks go to the Autonomous Driving & Intelligent Transport group at Oxford Brookes University, and the OBR\\nAutonomous team for their input and assistance. The authors would also like to thank Salman Khan, Peter Ball,\\nTjeerd Olde Scheper, Matthias Rolf, Alex Rast, and Gordana Collier for their ongoing support.\\nThis project has received funding from the European Union’s Horizon 2020 research and innovation programme, under\\ngrant agreement No. 964505 (E-pi).\\nReferences\\n[1] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. YOLOv4: Optimal Speed and Accuracy of Object\\nDetection. apr 2020.\\n[2] Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao, Guangming Shi, and Jinjian Wu. Feature-fused ssd: Fast detection\\nfor small objects. In Ninth International Conference on Graphic and Image Processing (ICGIP 2017) , volume 10615, page\\n106151E. International Society for Optics and Photonics, 2018.\\n[3] Chenyi Chen, Ming-Yu Liu, Oncel Tuzel, and Jianxiong Xiao. R-CNN for Small Object Detection.Lecture Notes in Computer\\nScience (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 10115 LNCS:214–\\n230, nov 2016.\\n[4] Jacob Culley, Sam Garlick, Enric Gil Esteller, Petar Georgiev, Ivan Fursa, Isaac Vander Sluis, Peter Ball, and Andrew Bradley.\\nSystem design for a driverless autonomous racing vehicle. 2020 12th International Symposium on Communication Systems,\\nNetworks and Digital Signal Processing (CSNDSP), pages 1–6, 2020.\\n[5] Ross Girshick. Fast r-cnn, 2015.\\n[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and\\nsemantic segmentation Tech report (v5).\\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial Pyramid Pooling in Deep Convolutional Networks for\\nVisual Recognition. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and\\nLecture Notes in Bioinformatics), 8691 LNCS(PART 3):346–361, jun 2014.\\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. Proceedings of\\nthe IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December:770–778, dec 2015.\\n[9] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely Connected Convolutional Networks.\\nTechnical report.', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='9772f102-baa6-4192-9310-a24df4235e32', embedding=None, metadata={'page_label': '11', 'file_name': 'Doc1.pdf', 'file_path': '/content/drive/MyDrive/Projects/RAG_HandsOn/RAGwithLlama/Articles/Doc1.pdf', 'file_type': 'application/pdf', 'file_size': 4053803, 'creation_date': '2024-11-14', 'last_modified_date': '2024-11-09'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"[10] Wei Jia, Shiquan Xu, Zhen Liang, Yang Zhao, Hai Min, Shujie Li, and Ye Yu. Real-time automatic helmet detection of\\nmotorcyclists in urban trafﬁc using improved YOLOv5 detector. IET Image Processing, page ipr2.12295, jun 2021.\\n[11] Glenn Jocher, Alex Stoken, Jirka Borovec, NanoCode012, Ayush Chaurasia, TaoXie, Liu Changyu, Abhiram V , Laughing,\\ntkianai, yxNONG, Adam Hogan, lorenzomammana, AlexWang1900, Jan Hajek, Laurentiu Diaconu, Marc, Yonghye Kwon,\\noleg, wanghaoyang0106, Yann Defretin, Aditya Lohia, ml5ah, Ben Milanko, Benjamin Fineran, Daniel Khromov, Ding\\nYiwei, Doug, Durgesh, and Francisco Ingham. ultralytics/yolov5: v5.0 - YOLOv5-P6 1280 models, AWS, Supervise.ly and\\nYouTube integrations, Apr. 2021.\\n[12] Mate Kisantal, Zbigniew Wojna, Jakub Murawski, Jacek Naruniec, and Kyunghyun Cho. Augmentation for small object\\ndetection, 2019.\\n[13] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks\\nfor Object Detection. dec 2016.\\n[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan,\\nC. Lawrence Zitnick, and Piotr Doll ´ar. Microsoft COCO: Common Objects in Context. Lecture Notes in Computer Science\\n(including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 8693 LNCS(PART 5):740–\\n755, may 2014.\\n[15] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation, 2018.\\n[16] Yifan Liu, Binghang Lu, Jingyu Peng, and Zihao Zhang. Research on the Use of YOLOv5 Object Detection Algorithm in\\nMask Wearing Recognition. World Scientiﬁc Research Journal, 6:2020.\\n[17] Will Maddern, Geoff Pascoe, Chris Linegar, and Paul Newman. 1 Year, 1000km: The Oxford RobotCar Dataset. The\\nInternational Journal of Robotics Research (IJRR), 36(1):3–15, 2017.\\n[18] Valentina Mus, at, Ivan Fursa, Paul Newman, Fabio Cuzzolin, and Andrew Bradley. Multi-weather city: Adverse weather\\nstacking for autonomous driving. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2906–\\n2915, 2021.\\n[19] Nhat Duy Nguyen, Tien Do, Thanh Duc Ngo, and Duy Dinh Le. An Evaluation of Deep Learning Methods for Small Object\\nDetection. Journal of Electrical and Computer Engineering, 2020.\\n[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\\nNatalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\\nTejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\\nperformance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett,\\neditors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.\\n[21] Phuoc Pham, Duy Nguyen, Tien Do, Thanh Duc Ngo, and Duy-Dinh Le. Evaluation of Deep Models for Real-Time Small\\nObject Detection. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture\\nNotes in Bioinformatics), 10636 LNCS:516–526, nov 2017.\\n[22] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013–2016.\\n[23] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You Only Look Once: Uniﬁed, Real-Time Object\\nDetection. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2016-\\nDecember:779–788, jun 2015.\\n[24] Joseph Redmon and Ali Farhadi. YOLO9000: Better, Faster, Stronger. Proceedings - 30th IEEE Conference on Computer\\nVision and Pattern Recognition, CVPR 2017, 2017-January:6517–6525, dec 2016.\\n[25] Joseph Redmon and Ali Farhadi. YOLOv3: An Incremental Improvement. apr 2018.\\n[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region\\nproposal networks, 2016.\\n[27] Bharat Singh and Larry S. Davis. An Analysis of Scale Invariance in Object Detection SNIP, 2018.\\n[28] Bharat Singh, Mahyar Najibi, and Larry S. Davis. SNIPER: Efﬁcient Multi-Scale Training. Advances in Neural Information\\nProcessing Systems, 2018-December:9310–9320, may 2018.\\n[29] Bharat Singh, Mahyar Najibi, Abhishek Sharma, and Larry S Davis. Scale Normalized Image Pyramids with AutoFocus for\\nObject Detection.\\n[30] Mingxing Tan, Ruoming Pang, and Quoc V . Le. EfﬁcientDet: Scalable and Efﬁcient Object Detection. Proceedings of the\\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 10778–10787, nov 2019.\\n[31] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer. SqueezeDet: Uniﬁed, Small, Low Power Fully Convolutional\\nNeural Networks for Real-Time Object Detection for Autonomous Driving.\\n[32] Bin Yan, Pan Fan, Xiaoyan Lei, Zhijie Liu, and Fuzeng Yang. A Real-Time Apple Targets Detection Method for Picking\\nRobot Based on Improved YOLOv5. Remote Sensing 2021, Vol. 13, Page 1619, 13(9):1619, apr 2021.\\n[33] Guanhao Yang, Wei Feng, Jintao Jin, Qujiang Lei, Xiuhao Li, Guangchao Gui, and Weijun Wang. Face Mask Recognition\\nSystem with YOLOV5 Based on Image Recognition. 2020 IEEE 6th International Conference on Computer and Communi-\\ncations, ICCC 2020, pages 1398–1404, dec 2020.\\n[34] Fangbo Zhou, Huailin Zhao, and Zhen Nie. Safety Helmet Detection Based on YOLOv5. Proceedings of 2021 IEEE Inter-\\nnational Conference on Power Electronics, Computer Applications, ICPECA 2021, pages 6–11, jan 2021.\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"]},"metadata":{},"execution_count":9}],"source":["print(type(documents))\n","documents"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1731550679633,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"sq9cUALIUWkT","outputId":"49e5a687-4bd1-4fb1-e825-c174d8124d3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["11\n"]}],"source":["print(len(documents))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1731550679633,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"fhfALeM9UWkU","outputId":"f252cec8-edbc-4b4d-b0ca-63ea90007847"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------------------------\n","YOLO-Z: Improving small object detection in YOLOv5 for\n","autonomous vehicles\n","Aduen Benjumea* Izzeddin Teeti† Fabio Cuzzolin† Andrew Bradley*\n","17065125@brookes.ac.uk, 19136994@brookes.ac.uk,\n","fabio.cuzzolin@brookes.ac.uk, abradley@brookes.ac.uk\n","Abstract\n","As autonomous vehicles and autonomous racing rise in popularity, so does the need for faster and more accurate\n","detectors. While our naked eyes are able to extract contextual information almost instantly, even from far away,\n","image resolution and computational resources limitations make detecting smaller objects (that is, objects that occupy\n","a small pixel area in the input image) a genuinely challenging task for machines and a wide-open research ﬁeld. This\n","study explores how the popular YOLOv5 object detector can be modiﬁed to improve its performance in detecting\n","smaller objects, with a particular application in autonomous racing. To achieve this, we investigate how replacing\n","certain structural elements of the model (as well as their connections and other parameters) can affect performance\n","and inference time. In doing so, we propose a series of models at different scales, which we name ‘YOLO-Z’, and\n","which display an improvement of up to 6.9% in mAP when detecting smaller objects at 50% IOU, at the cost of\n","just a 3ms increase in inference time compared to the original YOLOv5. Our objective is to inform future research\n","on the potential of adjusting a popular detector such as YOLOv5 to address speciﬁc tasks and provide insights on\n","how speciﬁc changes can impact small object detection. Such ﬁndings, applied to the broader context of autonomous\n","vehicles, could increase the amount of contextual information available to such systems.\n","1 Introduction\n","Detecting small objects in images can be challenging, mainly due to limited resolution and context information avail-\n","able to a model [2]. Many modern systems that implement object detection do so at real-time speeds, setting speciﬁc\n","requirements in computational resources, especially if the processing is to happen on the same device that captures\n","the images. This is the case for many autonomous vehicle systems [4], where the vehicle itself captures and processes\n","images in real-time, often to inform its next actions. In this context, detecting smaller objects means detecting objects\n","farther away from the car, thus allowing earlier detection of such objects, effectively expanding the detection range of\n","the vehicle. Improvements in this speciﬁc area would better inform the system, allowing it to make more robust and\n","viable decisions.\n","Due to the nature of object detectors, the details of smaller objects lose signiﬁcance as they are processed by each\n","layer of their convolutional backbone. In this study, by ‘small objects’, we refer to objects which occupy a small pixel\n","area in the input image.\n","Efforts have been made to improve the detection of smaller objects [19], but many revolve around directing the pro-\n","cessing around a speciﬁc area of the image [29, 28, 27] or are focused around two-stage detectors, which are known for\n","achieving better performance at the cost of inference time, making them less suited for real-time applications. This is\n","also the reason why so many single-stage detectors have been developed for this type of applications [31]. Increasing\n","the input image resolution is another obvious way to bypass this issue which results, however, in a signiﬁcant increase\n","in processing time.\n","YOLOv5 is a very popular single-stage object detector [11] known for its performance and speed with a clear and\n","ﬂexible structure that can be broken down, adjusted and built on a very widely accessible platform. Many of the\n","systems that apply this architecture and attempt to optimise it, however, they mainly rely on adjusting speciﬁc param-\n","eters or augmenting their training set to improve performance [33], without much consideration for structural changes\n","*Visual Artiﬁcial Intelligence Laboratory, Oxford Brookes University, UK\n","†Autonomous Driving & Intelligent Transport Group, Oxford Brookes University, UK\n","1\n","arXiv:2112.11798v4  [cs.CV]  3 Jan 2023\n","----------------------------------------------------------------------------------\n","to the model itself to better adapt it for a speciﬁc use case. While YOLOv5 is a potent tool, it is designed to be a\n","general-purpose object detector and therefore is not optimised to detect smaller objects.\n","This study proposes ways in which YOLOv5 can be modiﬁed to better perform on a given system in terms of small\n","object detection, with clear real-world implications [4]. We consider, in particular, the case of an autonomous racing\n","vehicle that needs to detect differently coloured cones to drive around a track. We will discuss the effects of differ-\n","ent techniques and propose modiﬁed models able to perform this task better while maintaining real-time processing\n","speeds. The contributions of this paper are:\n","1. A modiﬁed model of YOLOv5 speciﬁcally designed for better detections of small objects.\n","2. Proposing a methodology to modify the structure of YOLOv5 to improve performance in a particular task. This\n","is done in an experimental manner, analysing the different elements that make YOLOv5.\n","2 Related work\n","This study aims at reﬁning the already existing YOLOv5 model to deal with the many problems associated with\n","small object detection. This task is a complex area of machine learning that very quickly escalates in complexity as\n","requirements evolve. To work with such systems, it is essential to understand the bases upon which they are built, the\n","many different technologies and techniques that form the current state of the art and the related use cases.\n","2.1 One-stage vs two-stage objects detectors\n","We know we can classify object detectors into two categories: one-stage and two-stage detectors [19]. The latter\n","typically decomposes the detection task into (i) region proposal generation and (ii) classiﬁcation, as is the case with\n","Faster R-CNN and its predecessors [6, 5, 26]. While there have been efforts to improve the small object detection\n","ability of such models [3], a lot of the attention has been put on performance regardless of inference time. Two stage\n","detectors have however improved signiﬁcantly over time by streamlining their structure and data ﬂow.\n","2.2 The YOLO family\n","As a family of object detectors, YOLO takes this idea a step further and has grown very popular over the last few years.\n","With YOLOv1 [23], object detection is presented as a regression task, thus simplifying the networks and allowing us\n","to build faster models that can be used in real-time. Later versions of YOLO improve different aspects of the model\n","[23, 24, 25, 1]. Most notably, much effort has been spent on the backbone through the different versions. This begs\n","the question: What potential is there still untapped if changes to an isolated element can have such an impact?\n","YOLOv5 [11] was released very shortly after YOLOv4 [1]. Despite its name, the authors are not directly related,\n","and there have been discussions on whether it is fair to call YOLOv5 a successor of YOLOv4. This implementation\n","provides similar performance to YOLOv4 and shares the same design. The main point of attention is the fact that it\n","is fully written in the PyTorch framework [20] as opposed to using any form of the Darknet framework [22] and has\n","a focus on accessibility and use in a wider range of development environments. Additionally, the models in YOLOv5\n","prove to be signiﬁcantly smaller, faster to train and more accessible to be used in a real-world application.\n","2.3 Systems using and modifying YOLOv5\n","YOLO has been used in many applications requiring the detection of objects. In safety helmet detection systems\n","[34], for instance, YOLO can be adjusted and implemented in series with the rest of a system. Similarly, face masks\n","detectors have been seen at the entrances of metro stations [33]. Both of these applications do a good job at exploiting\n","the beneﬁts of YOLO for the detection of smaller objects [21], but do not go as far as modifying the architecture.\n","Other systems that do make an effort to optimize YOLOv5 do so in a limited fashion. Once again, mask detectors [16]\n","have been proposed that leverage anchors generated and data augmentation to ﬁt a model to the use case better. More\n","----------------------------------------------------------------------------------\n","complex systems for helmet detection [10] also do a great job at leveraging the contextual information around small\n","objects to isolate them and facilitate their detection. However, their approach is not quite universally applicable and\n","comes at the cost of introducing a two-step process.\n","Typical adjustments to the internal structures of the model are surface-level. In a recent apple detection system [32],\n","the backbone of YOLOv5 is slightly modiﬁed to simplify it, which offers the potential to adapt to the system’s re-\n","quirements and one that opens the way for additional changes. If a single backbone element is modiﬁed, more drastic\n","changes can be applied for additional effects.\n","2.4 Small object detection\n","Some effort has been put into developing systems which direct the processing towards certain areas of the input image\n","[29, 28, 27], which allows us to adjust resolution and therefore bypass the limitation of having fewer pixels deﬁning an\n","object. This approach, however, is better suited for systems that are not time-sensitive, as they require multiple passes\n","through a network at different scales. This idea of paying more attention to speciﬁc scales can nevertheless inspire the\n","way we treat certain feature maps.\n","Additionally, a lot can be learned by looking at how feature maps can be treated instead of just modifying the backbone.\n","Different types of feature pyramid networks (FPN) [13, 30, 15] can aggregate feature maps differently to enhance a\n","backbone in different ways. Such techniques prove to be rather effective.\n","2.5 Autonomous vehicles\n","Within autonomous driving, object detection can provide valuable contextual information about the vehicle’s surround-\n","ings and heavily inform its decision making process [17, 4]. In this case, smaller objects translate to objects further\n","away, meaning a more complete context for the system to make use of. These systems heavily focus on inference time,\n","sacriﬁcing performance if needed, but work can be done to improve them at minimal cost. Performance in this ﬁeld is\n","critical, as a small improvement in this system can greatly impact the entire vehicle. A common requirement in this\n","area is for detectors to be single-stage [31], for the simple reason that fewer steps and transitions between them often\n","translates into fewer resources needed.\n","3 Methodology\n","YOLOv5 provides four different scales for their model, S, M, L and X which stand for Small, Medium, Large, and\n","Xlarge, respectively. Each of these scales applies a different multiplier to the depth and width of the model, meaning\n","the overall structure of the model remains constant, but the size and complexity of each model are scaled. In Our\n","experiments, we apply changes to the structure of the models individually across all the scales and treat each one as a\n","different model for the purposes of evaluating their effect.\n","To set a baseline, we trained and tested the unmodiﬁed versions of the four scales of YOLOv5. We then tested changes\n","to these networks individually in order to observe their impact separately against our baseline results. The techniques\n","and structures that did not appear to contribute to better accuracy or inference time were ﬁltered out when moving to the\n","next phase. We then attempted combinations of the selected techniques. This process was repeated, observing whether\n","certain techniques complemented or diminished each other and adding more complex combinations progressively.\n","We ﬁrst discuss the appropriate evaluation metric for our work (Section 3.1), and the dataset used for our investigation\n","(Section 3.2). We then move on to describe our plans to apply a number of model changes to be run under controlled\n","circumstances (Section 3.2), logging and adjusting as we move through different stages.\n","3.1 Evaluation metric\n","The original implementation of YOLOv5 provides compatibility with Microsoft Common Objects in Context (COCO)\n","API’s [14] metrics at three different object scales (bounding box areas) and Intersection over Unions (IOU ), which\n","proves useful for the purpose of this study. The way values at speciﬁc scales are calculated can give us a good indication\n","----------------------------------------------------------------------------------\n","of the performance of the model, but may be slightly inaccurate in extreme cases, which will not be a problem for the\n","most part, but must be kept in mind.\n","Since these metrics are only compatible with the COCO dataset by default, we have re-implemented this method in\n","our testing code in order to obtain more valuable ﬁgures for our study while using any dataset. Our metric module\n","will calculate values for large, medium and small objects, in addition to the overall performance. The categorisation\n","of objects into these three categories depends on the following thresholds: ‘small’, if the object occupy an area less\n","than 32 squared pixels, ‘large’, if the area is more than 96 squared pixels, and ‘medium’, for anything between the two\n","thresholds. In other words, small < 322 < medium < 962 < large.\n","3.2 Dataset and Experimental setup\n","Figure 1: Dataset class instances\n","To train our models and inform our experiments we adopted\n","a dataset of annotated cones from the perspective of an au-\n","tonomous racing car. Its original purpose is to help plan a path\n","for an autonomous racing vehicle based on the colours of the\n","cones, knowing that there are a total of 4 classes (yellow, blue,\n","orange and big orange cones) and close to 4,000 images (see\n","Figure 1, 2). This dataset includes digitally augmented images\n","[18] and cases with challenging weather conditions. A dataset\n","such as this one models more complex tasks in autonomous\n","vehicles. Cones are themselves objects that we would ﬁnd on\n","the road and share many qualities with other objects such as of\n","trafﬁc signs in terms of size and position.\n","Although the dataset would beneﬁt from a larger size, it is\n","characterised by a very high object density, with over 30,000\n","labelled objects. Furthermore, looking at Figure 1, we can ob-\n","serve a very high bias towards the blue and yellow classes.\n","This makes sense as they serve to mark the two sides of the\n","racing track, but it does constitute an imbalance that will af-\n","fect the overall results (see Section 4). The performance on\n","these classes will be taken into account when evaluating the\n","models, namely by averaging the scores of the most prominent\n","classes.\n","Cones are naturally small objects already in comparison to\n","other objects commonly found in the autonomous driving scenario, such as other vehicles or pedestrians. The correl-\n","ogram (a chart of correlation statistics) in Figure 3 shows the position, width, and height of the bounding boxes of the\n","objects (cones) in the dataset. Our dataset features a high concentration of smaller object boxes, slightly elongated as\n","to be expected because of perspective projection. This high proportion of small objects makes it beneﬁcial for this type\n","of study, as it largely overcomes the issue with a lack of such objects in other popular datasets including MS COCO\n","[12].\n","Figure 2: Sample image from dataset\n","The dataset was split into training, validation and testing with\n","a ratio of 65:15:20. The validation set then informs the training\n","of the model, but is not as relevant as the other two, hence the\n","lower size.\n","The training for all the experiments was executed in an envi-\n","ronment that has 4 Nvidia GTX 1080 GPUs, each has 12 GB\n","VRAM. For testing, however, we used a single GTX1080TI\n","GPU with a batch size of 1, and an i7-6900K CPU working at\n","3.20GHz.\n","----------------------------------------------------------------------------------\n","3.3 Proposed architectural changes\n","Figure 3: Relation between the position (in x and y value of the center\n","point), width and height of instances of the dataset\n","Figure 4: YOLOv5 default structure. In the text we refer to\n","the elements of this architecture modiﬁed in our work.\n","YOLOv5 uses a yaml ﬁle to instruct a parser how to\n","build a model. We use this setup to write our own high-\n","level instructions on how different building blocks of the\n","model are built and with what parameters, hence modify-\n","ing its structure. to implement new structures we arrange\n","and give parameters to each building block or layer and\n","instruct the parser on how to build it if necessary. In our\n","words, we make use of the base and experimental net-\n","work blocks provided with YOLOv5, while implement-\n","ing additional blocks where needed to simulate the re-\n","quired structures.\n","3.3.1 Backbone\n","The backbone of a model is the element dedicated to tak-\n","ing the input image and extracting feature maps from it.\n","This is a crucial step in any object detector, as it is the\n","main structure responsible for extracting contextual in-\n","formation from the input image as well as for abstracting\n","that information into patterns. We experimented with re-\n","placing the existing backbone in YOLOv5 with two sep-\n","arate options. ResNet [8] is a popular structure that intro-\n","duces residual connections to lessen the effect of the di-\n","minishing return we observe in deeper neural networks.\n","DenseNet [9] uses similar connections to preserve as\n","much information as possible as it moves through the\n","network. Implementing these structures requires break-\n","ing them down to their fundamental blocks and ensuring\n","the layers communicate appropriately. This includes en-\n","suring the right feature map dimensions, which at times\n","requires slightly modifying the scaling factor for the\n","width and depth of the model.\n","In both cases, it was important to avoid drastically devi-\n","ating the number of layers from the original as to main-\n","tain a comparable complexity. Hence, ResNet50 was\n","used and we downscaled DenseNet proportionally so it reattains its core functionality.\n","Additionally, YOLOv5 makes use of a Spatial Pyramid Pooling (SPP) [7] layer in between the backbone and the neck.\n","In our work, however, we have maintained this layer untouched.\n","3.3.2 Neck\n","We term ‘neck’ the structure placed between the head and backbone (see Figure 4) whose objective is to aggregate as\n","much information extracted by the backbone as possible before it is fed to the head. This structure plays a major role\n","in transferring small-object information by preventing it from being lost to higher levels of abstraction. It does this by\n","upsampling the resolution of the feature maps once again so different layers from the backbone can be aggregated and\n","regain inﬂuence on the detection step. [15].\n","In this work, we simpliﬁed the current Pan-Net [15] to that of an FPN [13] and replaced it a with biFPN [30]. In\n","both cases the neck retains a similar functionality, but varies in complexity and therefore the number of layers and\n","connections required for their implementation.\n","----------------------------------------------------------------------------------\n","3.3.3 Other modiﬁcations\n","Figure 5: Example of how favoring smaller feature maps can be implemented\n","in terms of structure both inclusively and exclusively.\n","The head of the model is responsible for taking\n","feature maps and inferring the bounding boxes\n","and classes by taking in several aggregated feature\n","maps from the neck. This structure can remain un-\n","touched, other than the parameters it receives, as\n","it is a fundamental part of the model that does not\n","have as much impact in small object detection as\n","the aforementioned elements.\n","There are, however, other elements that can have\n","an impact on small object detection performance.\n","Other than input image size, the depth and width\n","of the model can be modiﬁed in order to change\n","what aspect of the network the bulk of the process-\n","ing goes towards. The way layers are connected\n","can also be manually altered in the neck and head\n","in order to focus on detecting certain feature maps.\n","In this study we explored the effect of redirecting\n","the connections involving higher-resolution fea-\n","ture maps, in order for them to be fed directly to\n","the neck and head. This can be done in an ‘inclu-\n","sive’ manner by expanding the neck to ﬁt an extra\n","feature map, or in an ‘exclusive’ fashion by replac-\n","ing the lowest-resolution feature map in order to ﬁt\n","the new one, Figure 5 shows both options, as well\n","as the default (original) layout. Making use of a\n","higher resolution feature map would usually im-\n","prove performance on smaller object at the cost of\n","inference time and potentially detection of larger\n","objects, similar to the effect of increasing input im-\n","age size. We integrate this behaviour in the neck in\n","these two ways to minimize the downsides while\n","making the most out of its beneﬁts.\n","Note that a number of parameters will have to be adjusted to the new structure, as the learning capabilities of the\n","network can be affected. Mainly, the sizes of the anchor boxes applied in the head, which need to adjust to the\n","resolution of the feature maps being used.\n","----------------------------------------------------------------------------------\n","Figure 6: Results of applying individual architectural changes to YOLOv5 at each scale. We report the mAp at 50% IOU across all objects\n","sizes (top), the mAP at 50% IOU for small objects only (middle), and the inference speed in frames per second (fps, bottom). YOLOv5: the\n","baseline. lr02: changing the learning rate to 0.02. lr005: changing the learning rate to 0.005. resnet50: changing the backbone to that of ResNet50.\n","densenet: changing the backbone to that of DenseNet. 3anch: auto-generating 3 anchors per scale. 5anch: auto-generating 5 anchors per scale.\n","fpn: changing neck to that of FPN. bifpn: changing neck to that of BiFPN. deep: increasing the depth modiﬁer to that of the next scale up (or\n","equivalent). wide: increasing the width modiﬁer to that of the next scale up (or equivalent). XSinc: setting an extra small feature map inclusively\n","(see 3.3.3). XSex: setting an extra small feature map exclusively (see 3.3.3).\n","4 Results\n","Model Features\n","YOLO-Z S DenseNet, FPN, 3 anchors, extra small exclusive feature map\n","YOLO-Z M DenseNet, FPN, 5 anchors, extra small exclusive feature map\n","YOLO-Z L DenseNet, FPN, 5 anchors, extra small exclusive feature map\n","YOLO-Z X DenseNet, bi-FPN, 5 anchors, extra small exclusive feature map\n","Table 1: Modiﬁcations applied to YOLOv5 to achieve models of the YOLO-Z family. Each scale uses its YOLOv5 equivalent as a base.\n","mAP .5 mAP .5 small inference (ms)\n","Scales YOLOv5 YOLO-Z difference YOLOv5 YOLO-Z difference YOLOv5 YOLO-Z difference\n","S 0.926 0.955 3.13% 0.869 0.925 6.44% 8 8.9 0.9\n","M 0.932 0.9605 3.06% 0.8795 0.9425 7.16% 11.6 14.3 2.7\n","L 0.935 0.964 3.10% 0.886 0.9545 7.73% 16.6 19.6 3\n","X 0.9385 0.9605 2.34% 0.8975 0.9465 5.46% 26.9 30.6 3.7\n","Table 2: Comparing performance and inference time of YOLOv5 and YOLO-Z (optimal values for each scale in bold).\n","Note that we only show performance on the yellow and blue classes, as they are the best represented in the dataset\n","according to Figure 1. (See supplementary material ”Individual test results”, Table 1).\n","4.1 Inﬂuence of the backbone\n","A comparison of the performance of the two backbones (see Figure 6) shows that DenseNet consistently exhibits a\n","signiﬁcant improvement at what appears to be a relatively low ﬁxed increase in inference time (about 3 ms). ResNet\n","not only seems to worsen performance in most cases, but its inference time is also signiﬁcantly higher, leaving no\n","reason to consider it further at this stage. Our conclusion is that DenseNet is therefore a better ﬁt, in general, for small\n","scale object detection. In the smaller scale models, this can be due to not having networks deep enough to reap the\n","beneﬁts of a ResNet backbone, while DenseNet does a good job at preserving feature maps’ details.\n","----------------------------------------------------------------------------------\n","Figure 7: Performance comparison between the YOLOv5 and YOLO-Z families of models, plotting mAP (top), mAP for small objects (middle)\n","and mAP for medium objects (bottom) against inference time (ms). Clearly the superior average performance of YOLO-Z is achieved at the smaller\n","scale, while performance is stable and very close to 1 at the medium scale.\n","4.2 Inﬂuence of neck architecture\n","Using an FPN only outperforms bi-FPN at the S scale (see Figure 6). In the latter case inference time remains fairly\n","comparable to that of the original YOLOv5 neck, which is not too surprising given their similarities. This might\n","suggest that simpler models beneﬁt from keeping the feature maps relatively untouched, while other scales require\n","extra steps to adapt to the added processing of the feature maps and eventually outperform the former.\n","4.3 Feature maps\n","In our experiments, redirecting what feature maps are fed to the neck and head had the most signiﬁcant impact among\n","all techniques. Excluding the lowest resolution feature map in order to replace it with a higher-resolution one ( XS ex\n","in Figure 6) proved particularly effective. This can be attributed to the fact that, after including a higher resolution\n","map in the head, small objects end up occupying more pixels and having therefore more of an inﬂuence, rather than\n","being ‘lost’ in the convolution stages of the backbone. Similarly, getting rid of the original lower-resolution feature\n","map reduces the amount of processing needed and prevents the model from counteracting the level of detail provided\n","by the higher-resolution map. This is likely a consequence of the dataset used having a very high density of small\n","objects; performance will likely vary signiﬁcantly in other applications. The only exception to this pattern seems to\n","be that of the extra large scale ( X), for which the improvement is not as signiﬁcant and keeping the lower resolution\n","feature maps actually appears detrimental to performance In comparison to the baseline.\n","4.4 Inﬂuence of the number of anchors\n","Letting YOLO generate anchors based on the dataset provided proves effective in performance without affecting\n","inference time. However, The magnitude of the effect and the number of anchors a model favors does seem to be\n","affected by scale (anch3 and anch5 in Figure 6). Again, the dataset used is relevant to this step as, in our tests, most\n","cones will have a similar elongated shape on the y axis (see Figure 3). Other applications with objects varying more\n","in shape might ﬁnd different results.\n","In terms of scale, the smaller models tend to beneﬁt from fewer anchors, while the opposite is true for the larger scales.\n","Namely, at the S scale having 3 anchors outperforms having 5, while the gap reduces at the M scale. Models L and\n","X, on the other hand, display instead a better performance at 5 anchors. This suggests that more complex or deeper\n","models may indeed beneﬁt from additional anchors or, in other words, may be more capable of taking advantage of\n","the details additional anchors provide.\n","----------------------------------------------------------------------------------\n","4.5 Other factors\n","Figure 8: Visual demonstration of the improved detec-\n","tion results of YOLO-Z S (bottom) compared to YOLOv5\n","S (top) over a region of a sample image covering far away /\n","small scale objects. Yellow and blue cone detections are\n","shown as bounding boxes in the respective colours, de-\n","tections missed by both models are shown as red boxes,\n","detections missed by YOLOv5 but correctly identiﬁed by\n","YOLO-Z S as red circles. One can observe that the im-\n","provement is most evident for farther away / smaller cones.\n","In addition to the other major structural changes, a larger learning\n","rate proved to better leverage the models, but this can vary with the\n","number of epochs the latter are trained for (lr02 and lr005 in Figure\n","6).\n","Interestingly, a wider model (higher width multiplier) showed to\n","have a positive effect on the smaller scales as opposed to a deeper\n","one ( deep and wide in Figure 6). The opposite is true for the L\n","scale. This might be due to the speciﬁc characteristics of our dataset,\n","in which the objects to be detected form relatively simple patterns\n","with rather similar features. Nevertheless, more testing would be\n","needed to determine this, as this pattern is not continued in the extra\n","large scale, where both modiﬁcations harm performance by the same\n","amount. Additionally, these types of alterations do have a noticeable\n","negative effect on inference speed, discouraging their use.\n","4.6 Modiﬁed models\n","Additional tests were carried out using various combinations of the\n","aforementioned alteration techniques in order to seek models that\n","further deviate from the originals but, at the same time, can further\n","improve performance. We refer to this proposed family of models at\n","different scales as YOLO-Z, short for ‘YOLO Zoomed’ (see Table\n","1).\n","A comparison of the performance of these new models shows that\n","an FPN neck tends to outperform bi-FPN (compare YOLO-Z X with\n","the other YOLO-Z models in Table 2) for scales where the opposite\n","was previously true. Aside from this, we could only observe small\n","variations across the models. An exception is the X scale, which\n","seems to gain less from such changes and even with the use of a\n","different neck structure does not deliver improvements as signiﬁcant\n","as with the other scales.\n","This considered, YOLO-Z models achieved an average 2.7 performance increase in absolute mAP at 50% IoU for all\n","objects and an absolute improvement of 5.9 for small objects at the same IoU across all scales. This comes at the cost\n","of an average 2.6ms increase in inference time.\n","4.7 Discussion\n","In our investigation of ways in which a popular object detector such as YOLOv5 can be adapted to better detect smaller\n","objects, we were able identify architectural modiﬁcations delivering a clear improvement in performance compared to\n","original at relatively little cost, as the new models retain real-time inference speed.\n","The context in which we have applied the proposed techniques, that of autonomous racing, is one that can greatly\n","beneﬁt from such an improvement. As we can see in Figure 8, such changes do have a quantiﬁable impact on detection.\n","In this work we have not only signiﬁcantly improved the performance of the baseline model, but also identiﬁed a\n","number of speciﬁc techniques that can be applied to any other application involving the detection of small or far away\n","objects.\n","The net result is that models of the YOLO-Z family outperform those of the YOLOv5 class while retaining an inference\n","time compatible with a real time application such as autonomous racing (see Table 2 and Figure 7). This is especially\n","true for the smaller objects which have been the focus of this study (Figure 7, middle), whereas the performance is\n","stable for medium-sized objects (bottom).\n","----------------------------------------------------------------------------------\n","Note that, while we have focused here on modifying the popular YOLOv5 model, the methods and techniques we\n","explored have a potential to be developed into an entirely original model structure.\n","Finally, while this study shows signiﬁcant the empirical gains of the proposed architectural changes, the consistency\n","and generality of the results could and should be further investigated. For instance, the analysis would greatly beneﬁt\n","from further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\n","While we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\n","understood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\n","more robust solution to small object detection. Additionally, there many more directions and techniques that would ﬁt\n","nicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\n","5 Conclusions\n","In this study, we have investigated the effects and rationale of different architectural and model alterations applied\n","to the popular YOLOv5 object detector in order to improve its small-object detection abilities. We have validated\n","these techniques in the autonomous racing scenario, highlighting its speciﬁc needs and limitations, and outlining\n","possible further research. Doing so has produced an original YOLO-Z family of models capable of delivering an\n","improvement in the ability of detecting small objects (measured by the standard mAP at 50% IoU) close to 6%, while\n","only increasing inference time by around 3 ms. Using these ﬁndings existing systems can be upgraded to better detect\n","very small objects in situation in which current models cannot detect anything at all. This can extend an autonomous\n","vehicle’s detection range and perception robustness, leading to better planning and decision making strategies that can\n","give an autonomous racing car an important edge.\n","6 Acknowledgements\n","Thanks go to the Autonomous Driving & Intelligent Transport group at Oxford Brookes University, and the OBR\n","Autonomous team for their input and assistance. The authors would also like to thank Salman Khan, Peter Ball,\n","Tjeerd Olde Scheper, Matthias Rolf, Alex Rast, and Gordana Collier for their ongoing support.\n","This project has received funding from the European Union’s Horizon 2020 research and innovation programme, under\n","grant agreement No. 964505 (E-pi).\n","References\n","[1] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. YOLOv4: Optimal Speed and Accuracy of Object\n","Detection. apr 2020.\n","[2] Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao, Guangming Shi, and Jinjian Wu. Feature-fused ssd: Fast detection\n","for small objects. In Ninth International Conference on Graphic and Image Processing (ICGIP 2017) , volume 10615, page\n","106151E. International Society for Optics and Photonics, 2018.\n","[3] Chenyi Chen, Ming-Yu Liu, Oncel Tuzel, and Jianxiong Xiao. R-CNN for Small Object Detection.Lecture Notes in Computer\n","Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 10115 LNCS:214–\n","230, nov 2016.\n","[4] Jacob Culley, Sam Garlick, Enric Gil Esteller, Petar Georgiev, Ivan Fursa, Isaac Vander Sluis, Peter Ball, and Andrew Bradley.\n","System design for a driverless autonomous racing vehicle. 2020 12th International Symposium on Communication Systems,\n","Networks and Digital Signal Processing (CSNDSP), pages 1–6, 2020.\n","[5] Ross Girshick. Fast r-cnn, 2015.\n","[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and\n","semantic segmentation Tech report (v5).\n","[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial Pyramid Pooling in Deep Convolutional Networks for\n","Visual Recognition. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and\n","Lecture Notes in Bioinformatics), 8691 LNCS(PART 3):346–361, jun 2014.\n","[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. Proceedings of\n","the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December:770–778, dec 2015.\n","[9] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely Connected Convolutional Networks.\n","Technical report.\n","----------------------------------------------------------------------------------\n","[10] Wei Jia, Shiquan Xu, Zhen Liang, Yang Zhao, Hai Min, Shujie Li, and Ye Yu. Real-time automatic helmet detection of\n","motorcyclists in urban trafﬁc using improved YOLOv5 detector. IET Image Processing, page ipr2.12295, jun 2021.\n","[11] Glenn Jocher, Alex Stoken, Jirka Borovec, NanoCode012, Ayush Chaurasia, TaoXie, Liu Changyu, Abhiram V , Laughing,\n","tkianai, yxNONG, Adam Hogan, lorenzomammana, AlexWang1900, Jan Hajek, Laurentiu Diaconu, Marc, Yonghye Kwon,\n","oleg, wanghaoyang0106, Yann Defretin, Aditya Lohia, ml5ah, Ben Milanko, Benjamin Fineran, Daniel Khromov, Ding\n","Yiwei, Doug, Durgesh, and Francisco Ingham. ultralytics/yolov5: v5.0 - YOLOv5-P6 1280 models, AWS, Supervise.ly and\n","YouTube integrations, Apr. 2021.\n","[12] Mate Kisantal, Zbigniew Wojna, Jakub Murawski, Jacek Naruniec, and Kyunghyun Cho. Augmentation for small object\n","detection, 2019.\n","[13] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks\n","for Object Detection. dec 2016.\n","[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan,\n","C. Lawrence Zitnick, and Piotr Doll ´ar. Microsoft COCO: Common Objects in Context. Lecture Notes in Computer Science\n","(including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 8693 LNCS(PART 5):740–\n","755, may 2014.\n","[15] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation, 2018.\n","[16] Yifan Liu, Binghang Lu, Jingyu Peng, and Zihao Zhang. Research on the Use of YOLOv5 Object Detection Algorithm in\n","Mask Wearing Recognition. World Scientiﬁc Research Journal, 6:2020.\n","[17] Will Maddern, Geoff Pascoe, Chris Linegar, and Paul Newman. 1 Year, 1000km: The Oxford RobotCar Dataset. The\n","International Journal of Robotics Research (IJRR), 36(1):3–15, 2017.\n","[18] Valentina Mus, at, Ivan Fursa, Paul Newman, Fabio Cuzzolin, and Andrew Bradley. Multi-weather city: Adverse weather\n","stacking for autonomous driving. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2906–\n","2915, 2021.\n","[19] Nhat Duy Nguyen, Tien Do, Thanh Duc Ngo, and Duy Dinh Le. An Evaluation of Deep Learning Methods for Small Object\n","Detection. Journal of Electrical and Computer Engineering, 2020.\n","[20] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\n","Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan\n","Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-\n","performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett,\n","editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.\n","[21] Phuoc Pham, Duy Nguyen, Tien Do, Thanh Duc Ngo, and Duy-Dinh Le. Evaluation of Deep Models for Real-Time Small\n","Object Detection. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture\n","Notes in Bioinformatics), 10636 LNCS:516–526, nov 2017.\n","[22] Joseph Redmon. Darknet: Open source neural networks in c. http://pjreddie.com/darknet/, 2013–2016.\n","[23] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You Only Look Once: Uniﬁed, Real-Time Object\n","Detection. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , 2016-\n","December:779–788, jun 2015.\n","[24] Joseph Redmon and Ali Farhadi. YOLO9000: Better, Faster, Stronger. Proceedings - 30th IEEE Conference on Computer\n","Vision and Pattern Recognition, CVPR 2017, 2017-January:6517–6525, dec 2016.\n","[25] Joseph Redmon and Ali Farhadi. YOLOv3: An Incremental Improvement. apr 2018.\n","[26] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region\n","proposal networks, 2016.\n","[27] Bharat Singh and Larry S. Davis. An Analysis of Scale Invariance in Object Detection SNIP, 2018.\n","[28] Bharat Singh, Mahyar Najibi, and Larry S. Davis. SNIPER: Efﬁcient Multi-Scale Training. Advances in Neural Information\n","Processing Systems, 2018-December:9310–9320, may 2018.\n","[29] Bharat Singh, Mahyar Najibi, Abhishek Sharma, and Larry S Davis. Scale Normalized Image Pyramids with AutoFocus for\n","Object Detection.\n","[30] Mingxing Tan, Ruoming Pang, and Quoc V . Le. EfﬁcientDet: Scalable and Efﬁcient Object Detection. Proceedings of the\n","IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 10778–10787, nov 2019.\n","[31] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer. SqueezeDet: Uniﬁed, Small, Low Power Fully Convolutional\n","Neural Networks for Real-Time Object Detection for Autonomous Driving.\n","[32] Bin Yan, Pan Fan, Xiaoyan Lei, Zhijie Liu, and Fuzeng Yang. A Real-Time Apple Targets Detection Method for Picking\n","Robot Based on Improved YOLOv5. Remote Sensing 2021, Vol. 13, Page 1619, 13(9):1619, apr 2021.\n","[33] Guanhao Yang, Wei Feng, Jintao Jin, Qujiang Lei, Xiuhao Li, Guangchao Gui, and Weijun Wang. Face Mask Recognition\n","System with YOLOV5 Based on Image Recognition. 2020 IEEE 6th International Conference on Computer and Communi-\n","cations, ICCC 2020, pages 1398–1404, dec 2020.\n","[34] Fangbo Zhou, Huailin Zhao, and Zhen Nie. Safety Helmet Detection Based on YOLOv5. Proceedings of 2021 IEEE Inter-\n","national Conference on Power Electronics, Computer Applications, ICPECA 2021, pages 6–11, jan 2021.\n"]}],"source":["for doc in documents:\n","    print(\"----------------------------------------------------------------------------------\")\n","    print(doc.text)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1731550679633,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"Nd9o_zTCsLJV","outputId":"c186bc12-ed57-4a0c-942c-9998c7626b69"},"outputs":[{"output_type":"stream","name":"stdout","text":["Note that, while we have focused here on modifying the popular YOLOv5 model, the methods and techniques we\n","explored have a potential to be developed into an entirely original model structure.\n","Finally, while this study shows signiﬁcant the empirical gains of the proposed architectural changes, the consistency\n","and generality of the results could and should be further investigated. For instance, the analysis would greatly beneﬁt\n","from further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\n","While we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\n","understood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\n","more robust solution to small object detection. Additionally, there many more directions and techniques that would ﬁt\n","nicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\n","5 Conclusions\n","In this study, we have investigated the effects and rationale of different architectural and model alterations applied\n","to the popular YOLOv5 object detector in order to improve its small-object detection abilities. We have validated\n","these techniques in the autonomous racing scenario, highlighting its speciﬁc needs and limitations, and outlining\n","possible further research. Doing so has produced an original YOLO-Z family of models capable of delivering an\n","improvement in the ability of detecting small objects (measured by the standard mAP at 50% IoU) close to 6%, while\n","only increasing inference time by around 3 ms. Using these ﬁndings existing systems can be upgraded to better detect\n","very small objects in situation in which current models cannot detect anything at all. This can extend an autonomous\n","vehicle’s detection range and perception robustness, leading to better planning and decision making strategies that can\n","give an autonomous racing car an important edge.\n","6 Acknowledgements\n","Thanks go to the Autonomous Driving & Intelligent Transport group at Oxford Brookes University, and the OBR\n","Autonomous team for their input and assistance. The authors would also like to thank Salman Khan, Peter Ball,\n","Tjeerd Olde Scheper, Matthias Rolf, Alex Rast, and Gordana Collier for their ongoing support.\n","This project has received funding from the European Union’s Horizon 2020 research and innovation programme, under\n","grant agreement No. 964505 (E-pi).\n","References\n","[1] Alexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. YOLOv4: Optimal Speed and Accuracy of Object\n","Detection. apr 2020.\n","[2] Guimei Cao, Xuemei Xie, Wenzhe Yang, Quan Liao, Guangming Shi, and Jinjian Wu. Feature-fused ssd: Fast detection\n","for small objects. In Ninth International Conference on Graphic and Image Processing (ICGIP 2017) , volume 10615, page\n","106151E. International Society for Optics and Photonics, 2018.\n","[3] Chenyi Chen, Ming-Yu Liu, Oncel Tuzel, and Jianxiong Xiao. R-CNN for Small Object Detection.Lecture Notes in Computer\n","Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics), 10115 LNCS:214–\n","230, nov 2016.\n","[4] Jacob Culley, Sam Garlick, Enric Gil Esteller, Petar Georgiev, Ivan Fursa, Isaac Vander Sluis, Peter Ball, and Andrew Bradley.\n","System design for a driverless autonomous racing vehicle. 2020 12th International Symposium on Communication Systems,\n","Networks and Digital Signal Processing (CSNDSP), pages 1–6, 2020.\n","[5] Ross Girshick. Fast r-cnn, 2015.\n","[6] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and\n","semantic segmentation Tech report (v5).\n","[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Spatial Pyramid Pooling in Deep Convolutional Networks for\n","Visual Recognition. Lecture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and\n","Lecture Notes in Bioinformatics), 8691 LNCS(PART 3):346–361, jun 2014.\n","[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. Proceedings of\n","the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2016-December:770–778, dec 2015.\n","[9] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely Connected Convolutional Networks.\n","Technical report.\n"]}],"source":["print(documents[-2].text)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1731550679633,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"b85ZZvK9r-vL","outputId":"a76ce7d9-084a-4bb0-b50d-377d592805f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Note that, while we have focused here on modifying the popular YOLOv5 model, the methods and techniques we\n","explored have a potential to be developed into an entirely original model structure.\n","Finally, while this study shows signiﬁcant the empirical gains of the proposed architectural changes, the consistency\n","and generality of the results could and should be further investigated. For instance, the analysis would greatly beneﬁt\n","from further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\n","While we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\n","understood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\n","more robust solution to small object detection. Additionally, there many more directions and techniques that would ﬁt\n","nicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\n","5 Conclusions\n","In this study, we have investigated the effects and rationale of different architectural and model alterations applied\n","to the popular YOLOv5 object detector in order to improve its small-object detection abilities. We have validated\n","these techniques in the autonomous racing scenario, highlighting its speciﬁc needs and limitations, and outlining\n","possible further research. Doing so has produced an original YOLO-Z family of models capable of delivering an\n","improvement in the ability of detecting small objects (measured by the standard mAP at 50% IoU) close to 6%, while\n","only increasing inference time by around 3 ms. Using these ﬁndings existing systems can be upgraded to better detect\n","very small objects in situation in which current models cannot detect anything at all. This can extend an autonomous\n","vehicle’s detection range and perception robustness, leading to better planning and decision making strategies that can\n","give an autonomous racing car an important edge.\n","6 Acknowledgements\n","Thanks go to the Autonomous Driving & Intelligent Transport group at Oxford Brookes University, and the OBR\n","Autonomous team for their input and assistance. The authors would also like to thank Salman Khan, Peter Ball,\n","Tjeerd Olde Scheper, Matthias Rolf, Alex Rast, and Gordana Collier for their ongoing support.\n","This project has received funding from the European Union’s Horizon 2020 research and innovation programme, under\n","grant agreement No. 964505 (E-pi).\n"]}],"source":["import re\n","\n","dropper = re.search(\"References(.*)\", documents[-2].text, re.DOTALL)\n","if dropper:\n","    dropper = dropper.group(1)\n","    # print(dropper)\n","    documents[-2].text = documents[-2].text.replace(\"References\"+dropper, \"\")\n","    documents[-2].text = documents[-2].text.strip()\n","print(documents[-2].text)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"stLxR7dZwjPz","executionInfo":{"status":"ok","timestamp":1731550679633,"user_tz":-330,"elapsed":11,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["del documents[-1]"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1731550679634,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"APFkOQEzyzRK","outputId":"e36f7645-7c90-404c-a78a-e74cc5050dd6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{},"execution_count":15}],"source":["len(documents)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"sDiDtc1hUWkU","executionInfo":{"status":"ok","timestamp":1731550680245,"user_tz":-330,"elapsed":620,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["index = VectorStoreIndex.from_documents(documents)"]},{"cell_type":"markdown","metadata":{"id":"w3D28iB5UWkV"},"source":["#### Setting up retriever"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"6ht0t4JNUWkV","executionInfo":{"status":"ok","timestamp":1731550680246,"user_tz":-330,"elapsed":20,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["# Number of docs to retireve\n","top_k = 3\n","\n","# configure retriever\n","retriever = VectorIndexRetriever(index=index, similarity_top_k=top_k)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"dQB_p9a2UWkV","executionInfo":{"status":"ok","timestamp":1731550680246,"user_tz":-330,"elapsed":19,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["# Assemble query engine\n","query_engine = RetrieverQueryEngine(\n","    retriever=retriever,\n","    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.5)])"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"t0ow6KpZUWkW","executionInfo":{"status":"ok","timestamp":1731550680246,"user_tz":-330,"elapsed":18,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["query = \"How is object detection process implemented?\"\n","response = query_engine.query(query)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1731550680246,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"vamM-8dfUWkW","outputId":"41182e46-d878-4e86-a388-f64cc204f077"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'For instance, the analysis would greatly beneﬁt\\nfrom further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\\nWhile we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\\nunderstood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\\nmore robust solution to small object detection.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}],"source":["response.source_nodes[0].text"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1731550680247,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"1wwQeobJUWkW","outputId":"6a679ed6-af60-4d9b-f5a4-ccd748717318"},"outputs":[{"output_type":"stream","name":"stdout","text":["Context: \n","For instance, the analysis would greatly beneﬁt\n","from further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\n","While we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\n","understood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\n","more robust solution to small object detection.\n","\n","1 Introduction\n","Detecting small objects in images can be challenging, mainly due to limited resolution and context information avail-\n","able to a model [2]. Many modern systems that implement object detection do so at real-time speeds, setting speciﬁc\n","requirements in computational resources, especially if the processing is to happen on the same device that captures\n","the images.\n","\n","Additionally, there many more directions and techniques that would ﬁt\n","nicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\n","5 Conclusions\n","In this study, we have investigated the effects and rationale of different architectural and model alterations applied\n","to the popular YOLOv5 object detector in order to improve its small-object detection abilities.\n","\n","\n"]}],"source":["context = \"Context: \\n\"\n","for i in range(top_k):\n","    context = context+response.source_nodes[i].text + \"\\n\\n\"\n","\n","print(context)"]},{"cell_type":"markdown","metadata":{"id":"D1RrWXY1UWkX"},"source":["### Import fine-tuned model"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"RVyTS7GgUWkX","executionInfo":{"status":"ok","timestamp":1731550689699,"user_tz":-330,"elapsed":4590,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["!pip install -q auto_gptq"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"5_45Pv6HUWkY","executionInfo":{"status":"ok","timestamp":1731550694003,"user_tz":-330,"elapsed":860,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\""]},{"cell_type":"code","execution_count":24,"metadata":{"id":"qyXaBxyyUWkY","executionInfo":{"status":"ok","timestamp":1731550696309,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["from huggingface_hub import login\n","\n","login(HF_TOKEN)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"JTn1pdJnUWkX","executionInfo":{"status":"ok","timestamp":1731550701768,"user_tz":-330,"elapsed":1625,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["# Load model directly\n","from peft import PeftModel, PeftConfig\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240,"referenced_widgets":["bef54fcfbdb6432c85f20a19c4615dd9","9ab89cb6ae6547b0b2683aedce4a3e5b","e361d03698ec44b28787f7d800a99b19","fa2ee7a7febd4a2d8e1038c90732e5ff","a4b6378bddbc446c9617baa2f9f490b7","b1abbfaf47e54641b23f1afd3530eab5","ab7f2fc563ef46788689b3e78e876957","9488d64446e946988a0072465bd116fa","edc1d512ed8e444fa3589f0f6888c6e5","575941dae0d548c482d6b6f8a13da225","66218e5c1294439e9280306823549b99","1f6ac56deb304f189a66925c8a1de65d","ac448f467f2a446baa383591f52ad100","a8819dbec9694b6fa098cc2eacfe8f88","290cf3003ba54c9bba193f3fbddcacb9","7135886c52944936aa321afe64514d39","b450e92117644588bbfd17fcae2d1c05","f99a6d9ab05f46d8aa5bc98f0d285716","858f4ca662cf4fda9ecbc58788e4e1c1","71d1d2c9a43e42978321b5028ba9cc81","7650507255ee4cb78315c201ec600d11","debe58919e83432bba8547551c21d703"]},"executionInfo":{"elapsed":81841,"status":"ok","timestamp":1731550839171,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"SybP4D3UUWkY","outputId":"f3580500-d08b-45e1-b8db-b4e783b15921"},"outputs":[{"output_type":"stream","name":"stderr","text":["`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:  36%|###5      | 1.48G/4.16G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef54fcfbdb6432c85f20a19c4615dd9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5006: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n","  warnings.warn(\n","`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","Some weights of the model checkpoint at TheBloke/Mistral-7B-Instruct-v0.2-GPTQ were not used when initializing MistralForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n","- This IS expected if you are initializing MistralForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing MistralForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6ac56deb304f189a66925c8a1de65d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:optimum.gptq.quantizer:Found modules on cpu/disk. Using Exllama/Exllamav2 backend requires all the modules to be on GPU. Setting `disable_exllama=True`\n"]}],"source":["# model_name = \"google/gemma-2-2b\"\n","model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, tokens = True)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","# model = model.to(device)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["4a44df350303441382757211cb71e563","682978a5f3c2405ebc5a13aae48940ec","af1fe59722ef4732949650cbf5602475","bc77158640044fafa0ca50d224857e76","9370228afab94e6181336be17d0c3652","f618136145854506921f7967da61a727","d4f1d3178a1f4a4c974877bb70eda4ef","6f709496b2d447ee9412de06f16b27af","8ee26d30832d4263af6381848905821b","292790716b6240d7a8a7da9cfd62f287","f03d2ab44d90407f9b9af9de1a63d374","49a5ef35a13a4277a397bd68d1cc7050","4779042393cc4009bec82fa3d738ea67","03d59e0e293a498998223cdb5ece9026","caf76092d4e3458094f486a06914b22e","f311f3ab3726436583faf7a30e595b8e","f98b2eaf13a9452d80d502e118dfa088","9d7ebc5a996e47688ecad2fedd64ab71","3f17a798a4904a46b67e9535320bec6c","ab8823a91bf5403a8cbcc3e7c9f2f2a6","20f1c8020740415fb890b9a86280e56c","c0cde9ba99834415bf49d56a5ca90bd7"]},"executionInfo":{"elapsed":2278,"status":"ok","timestamp":1731550841440,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"dYk8N_dkUWkY","outputId":"69ea9a1a-0e66-4e48-ded3-12595e22e537"},"outputs":[{"output_type":"display_data","data":{"text/plain":["adapter_config.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a44df350303441382757211cb71e563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["adapter_model.safetensors:   0%|          | 0.00/8.40M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a5ef35a13a4277a397bd68d1cc7050"}},"metadata":{}}],"source":["config = PeftConfig.from_pretrained(\"shawhin/shawgpt-ft\")\n","model = PeftModel.from_pretrained(model, \"shawhin/shawgpt-ft\")\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True)"]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRbmjfvmqyY2","executionInfo":{"status":"ok","timestamp":1731550879199,"user_tz":-330,"elapsed":589,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}},"outputId":"75f8e729-828b-4967-fc62-1126213f5bf5"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): MistralForCausalLM(\n","      (model): MistralModel(\n","        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n","        (layers): ModuleList(\n","          (0-31): 32 x MistralDecoderLayer(\n","            (self_attn): MistralSdpaAttention(\n","              (rotary_emb): MistralRotaryEmbedding()\n","              (k_proj): QuantLinear()\n","              (o_proj): QuantLinear()\n","              (q_proj): lora.QuantLinear(\n","                (base_layer): QuantLinear()\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","                (quant_linear_module): QuantLinear()\n","              )\n","              (v_proj): QuantLinear()\n","            )\n","            (mlp): MistralMLP(\n","              (act_fn): SiLU()\n","              (down_proj): QuantLinear()\n","              (gate_proj): QuantLinear()\n","              (up_proj): QuantLinear()\n","            )\n","            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n","            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n","          )\n","        )\n","        (norm): MistralRMSNorm((4096,), eps=1e-05)\n","      )\n","      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"X42QhZ3rIY9f"},"source":["#### Generate response (no context)"]},{"cell_type":"markdown","metadata":{"id":"xGybG1-65u6h"},"source":["#### Create a Prompt"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"TZiqjZ9DUWkZ","executionInfo":{"status":"ok","timestamp":1731550941002,"user_tz":-330,"elapsed":744,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["inst = \"\"\"DocDiscuss, functioning as a virtual reporter/consultant on youtube, communicates in clear, accesible language, escalating to technical depth upon request.It reacts to feedbacksaptly and ends reponse with its signation '-DocDiscuss'.\n","DocDiscuss will tailor the length of its response to match the viewers comment, providing concise knowledge to brief expressions of gratitude or feedback, thus keeping the interaction natuaral and engaging.\n","Please respond to the following column\"\"\"\n"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"D9aTBgVADrDp","executionInfo":{"status":"ok","timestamp":1731550942787,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["prompt_template = lambda comment: f'''[INST] {inst} \\n{comment}[/INST]'''"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"fBKo0e4uFrAC","executionInfo":{"status":"ok","timestamp":1731550944661,"user_tz":-330,"elapsed":707,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["comment = \"What methods have been used to improve the detection of smaller objects. \""]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1731550948758,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"8VO7Lwx3Gv__","outputId":"0b8241c5-7197-442e-f196-9dd40f474ec0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INST] DocDiscuss, functioning as a virtual reporter/consultant on youtube, communicates in clear, accesible language, escalating to technical depth upon request.It reacts to feedbacksaptly and ends reponse with its signation '-DocDiscuss'.\n","DocDiscuss will tailor the length of its response to match the viewers comment, providing concise knowledge to brief expressions of gratitude or feedback, thus keeping the interaction natuaral and engaging.\n","Please respond to the following column \n","What methods have been used to improve the detection of smaller objects. [/INST]\n"]}],"source":["prompt = prompt_template(comment)\n","print(prompt)"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1731550951493,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"},"user_tz":-330},"id":"b7W01kMBHbOx","outputId":"1d316a8e-f53d-42da-8004-fcd5e5971b47"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): MistralForCausalLM(\n","      (model): MistralModel(\n","        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n","        (layers): ModuleList(\n","          (0-31): 32 x MistralDecoderLayer(\n","            (self_attn): MistralSdpaAttention(\n","              (rotary_emb): MistralRotaryEmbedding()\n","              (k_proj): QuantLinear()\n","              (o_proj): QuantLinear()\n","              (q_proj): lora.QuantLinear(\n","                (base_layer): QuantLinear()\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","                (quant_linear_module): QuantLinear()\n","              )\n","              (v_proj): QuantLinear()\n","            )\n","            (mlp): MistralMLP(\n","              (act_fn): SiLU()\n","              (down_proj): QuantLinear()\n","              (gate_proj): QuantLinear()\n","              (up_proj): QuantLinear()\n","            )\n","            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n","            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n","          )\n","        )\n","        (norm): MistralRMSNorm((4096,), eps=1e-05)\n","      )\n","      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":34}],"source":["model.eval()"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLfnLcKeIkFv","outputId":"8933b0c4-fc73-4396-8602-3c4e1307b61d","executionInfo":{"status":"ok","timestamp":1731551270619,"user_tz":-330,"elapsed":316328,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]}],"source":["inputs = tokenizer(prompt, return_tensors=\"pt\")\n","output = model.generate(input_ids = inputs[\"input_ids\"].to(device),\n","                        attention_mask = inputs[\"attention_mask\"].to(device),\n","                        max_new_tokens = 280)"]},{"cell_type":"code","source":["print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jja9vXvrewVN","executionInfo":{"status":"ok","timestamp":1731551273487,"user_tz":-330,"elapsed":7,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}},"outputId":"ede0cbbe-b88a-4768-fd09-0b356d74ac45"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[    1,   733, 16289, 28793, 18402,  3278, 12804, 28725, 26945,   390,\n","           264,  8252, 19044, 28748,  6524,   517,   440,   356,   368, 28707,\n","          4276, 28725,  1960, 23860,   297,  3081, 28725,   932,   274,  1070,\n","          3842, 28725, 19491,  1077,   298, 10067,  8478,  3714,  2159, 28723,\n","          1313,   312,  9664,   298, 12139, 28713,  1459,   346,   304,  9675,\n","           312,  1405,   395,   871,  1492,   352, 16763,  9441,  3278, 12804,\n","          4135,    13,  9441,  3278, 12804,   622,  8675,   271,   272,  3575,\n","           302,   871,  2899,   298,  2918,   272, 24886,  4517, 28725,  7501,\n","          3078,   864,  4788,   298,  6817, 16948,   302, 26485,   442, 12139,\n","         28725,  5884,  7603,   272, 11186,  3044, 11042,   282,   304, 19639,\n","         28723,    13, 12069,  9421,   298,   272,  2296,  4419, 28705,    13,\n","          3195,  5562,   506,   750,  1307,   298,  4916,   272, 15109,   302,\n","          7000,  6697, 28723,   733, 28748, 16289, 28793,    13, 23809, 28725,\n","            13,    13, 28737, 28742, 28715,   347,  4610,   298,  3342,   456,\n","          9067, 28808,    13,    13,  1551,  4372,   574,  2996, 28725,   736,\n","           460,  2856,  5562,   369,   506,   750,  1307,   298,  4916,   272,\n","         15109,   302,  7000,  6697,   297,  5714,  3809,   288, 28723,  4003,\n","           460,   264,  1664, 28747,    13,    13, 28740, 28723, 22742,  3469,\n","          9457,  9804, 28747,  2957,  3024,  5562,  1259,   390,  8333,   895,\n","          5516, 28713, 28725,  5358, 15109, 28725,   304, 18087,  4423,  6933,\n","         28723,  2957,  9804,   541,  1316, 11976,   272,  9349,   304,  4916,\n","           272, 23446,   302,  7000,  6697, 28723,    13, 28750, 28723, 14972,\n","          5168, 18539, 28747, 14972,  5168, 18539, 28725,  1259,   390,  4221,\n","          1420,  1479, 25726, 12167,   325, 28743, 11348, 28713,   557,   506,\n","          4894,  1598,  9081,   297,  6705,   288,  7000,  6697,   297,  5714,\n","          6203, 28723, 26656, 28713,   541,  2822,   298,  9051,  4190,   369,\n","           460,  8598,   354,  6705,   288,  7000,  6697, 28725,  1019,   513,\n","          1395,  4190,   460,   459,  5061,  9141,   298,   272,  2930,  5421,\n","         28723,    13, 28770, 28723, 18317, 28733, 13742,  3809,   288, 28747,\n","         19422,  4038,  1871,   477,  5166,  3809,   288, 22946,  1218, 28725,\n","          1259,   390, 22023,   304,   351,  8671, 28725,   541,  1316,  4916,\n","           272, 15109,   302,  7000,  6697, 28723,  1263,  2757, 28725, 22023,\n","           349,  1179,   438,  6705,   288, 26930,  7467, 28725,  1312,   351,\n","          8671,   349,  1873,   438,  6705,   288,  2664, 18983,  1514,   594,\n","         28723,    13, 28781, 28723,  6077, 28733,   411,  2100,  3809,   288,\n","         28747,  6077, 28733,   411,  2100,  3809,   288,  9804,   541,  1316,\n","          4916,   272, 10948,   302,  5714,  6203, 28725,  2492,   378,  7089,\n","           298,  6705,  7000,  6697, 28723,  2957,  9804,   938, 18539,   298,\n","          9220,  1356,  1486, 28733,   411,  2100,  6203,   477,  2859, 28733,\n","           411,  2100,  1178, 28723,    13,    13, 28737,  3317,   456,  1871,\n","           349, 10865, 28808,  3169,   528,   873,   513]], device='cuda:0')\n"]}]},{"cell_type":"code","execution_count":37,"metadata":{"id":"84B7dMT7PFsz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731551546381,"user_tz":-330,"elapsed":715,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}},"outputId":"18754ee0-1c26-439e-ab67-88cd5217524d"},"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', '[', 'INST', ']', 'Doc', 'Dis', 'cuss', ',', 'functioning', 'as', 'a', 'virtual', 'reporter', '/', 'cons', 'ult', 'ant', 'on', 'you', 't', 'ube', ',', 'commun', 'icates', 'in', 'clear', ',', 'acc', 'es', 'ible', 'language', ',', 'escal', 'ating', 'to', 'technical', 'depth', 'upon', 'request', '.', 'It', 're', 'acts', 'to', 'feedback', 's', 'apt', 'ly', 'and', 'ends', 're', 'ponse', 'with', 'its', 'sign', 'ation', \"'-\", 'Doc', 'Dis', 'cuss', \"'.\", '\\n', 'Doc', 'Dis', 'cuss', 'will', 'tail', 'or', 'the', 'length', 'of', 'its', 'response', 'to', 'match', 'the', 'viewers', 'comment', ',', 'providing', 'conc', 'ise', 'knowledge', 'to', 'brief', 'expressions', 'of', 'gratitude', 'or', 'feedback', ',', 'thus', 'keeping', 'the', 'interaction', 'nat', 'uar', 'al', 'and', 'engaging', '.', '\\n', 'Please', 'respond', 'to', 'the', 'following', 'column', '', '\\n', 'What', 'methods', 'have', 'been', 'used', 'to', 'improve', 'the', 'detection', 'of', 'smaller', 'objects', '.', '[', '/', 'INST', ']', '\\n', 'Hi', ',', '\\n', '\\n', 'I', \"'\", 'd', 'be', 'happy', 'to', 'discuss', 'this', 'topic', '!', '\\n', '\\n', 'To', 'answer', 'your', 'question', ',', 'there', 'are', 'several', 'methods', 'that', 'have', 'been', 'used', 'to', 'improve', 'the', 'detection', 'of', 'smaller', 'objects', 'in', 'medical', 'imag', 'ing', '.', 'Here', 'are', 'a', 'few', ':', '\\n', '\\n', '1', '.', 'Advanced', 'image', 'processing', 'techniques', ':', 'These', 'include', 'methods', 'such', 'as', 'wave', 'let', 'transform', 's', ',', 'edge', 'detection', ',', 'and', 'morph', 'ological', 'operations', '.', 'These', 'techniques', 'can', 'help', 'enhance', 'the', 'contrast', 'and', 'improve', 'the', 'visibility', 'of', 'smaller', 'objects', '.', '\\n', '2', '.', 'Deep', 'learning', 'algorithms', ':', 'Deep', 'learning', 'algorithms', ',', 'such', 'as', 'conv', 'olut', 'ional', 'neural', 'networks', '(', 'C', 'NN', 's', '),', 'have', 'shown', 'great', 'promise', 'in', 'detect', 'ing', 'smaller', 'objects', 'in', 'medical', 'images', '.', 'CNN', 's', 'can', 'learn', 'to', 'identify', 'features', 'that', 'are', 'relevant', 'for', 'detect', 'ing', 'smaller', 'objects', ',', 'even', 'if', 'those', 'features', 'are', 'not', 'easily', 'visible', 'to', 'the', 'human', 'eye', '.', '\\n', '3', '.', 'Multi', '-', 'modal', 'imag', 'ing', ':', 'Comb', 'ining', 'information', 'from', 'multiple', 'imag', 'ing', 'modal', 'ities', ',', 'such', 'as', 'CT', 'and', 'M', 'RI', ',', 'can', 'help', 'improve', 'the', 'detection', 'of', 'smaller', 'objects', '.', 'For', 'example', ',', 'CT', 'is', 'good', 'at', 'detect', 'ing', 'calc', 'ifications', ',', 'while', 'M', 'RI', 'is', 'better', 'at', 'detect', 'ing', 'soft', 'tissue', 'les', 'ions', '.', '\\n', '4', '.', 'Super', '-', 'res', 'olution', 'imag', 'ing', ':', 'Super', '-', 'res', 'olution', 'imag', 'ing', 'techniques', 'can', 'help', 'improve', 'the', 'resolution', 'of', 'medical', 'images', ',', 'making', 'it', 'easier', 'to', 'detect', 'smaller', 'objects', '.', 'These', 'techniques', 'use', 'algorithms', 'to', 'recon', 'struct', 'high', '-', 'res', 'olution', 'images', 'from', 'low', '-', 'res', 'olution', 'data', '.', '\\n', '\\n', 'I', 'hope', 'this', 'information', 'is', 'helpful', '!', 'Let', 'me', 'know', 'if']\n"]}],"source":["print(tokenizer.batch_decode(output[0]))"]},{"cell_type":"markdown","metadata":{"id":"vuJn8_13MWjF"},"source":["#### Generate response (with context)"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"ipPF26F8Iyap","executionInfo":{"status":"ok","timestamp":1731551583076,"user_tz":-330,"elapsed":1137,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}}},"outputs":[],"source":["prompt_template_with_context = lambda context, comment: f'''[INST] {inst}\n"," \\n{context}\n"," Please respond to the following comment. Use the context above if it is helpful.\n"," \\n{comment}[/INST]'''"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"gCwYVQqYOxs0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731551594237,"user_tz":-330,"elapsed":1243,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}},"outputId":"7f6d3e29-6c36-4bc4-f5cb-267cea7035ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INST] DocDiscuss, functioning as a virtual reporter/consultant on youtube, communicates in clear, accesible language, escalating to technical depth upon request.It reacts to feedbacksaptly and ends reponse with its signation '-DocDiscuss'.\n","DocDiscuss will tailor the length of its response to match the viewers comment, providing concise knowledge to brief expressions of gratitude or feedback, thus keeping the interaction natuaral and engaging.\n","Please respond to the following column\n"," \n","Context: \n","For instance, the analysis would greatly beneﬁt\n","from further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\n","While we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\n","understood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\n","more robust solution to small object detection.\n","\n","1 Introduction\n","Detecting small objects in images can be challenging, mainly due to limited resolution and context information avail-\n","able to a model [2]. Many modern systems that implement object detection do so at real-time speeds, setting speciﬁc\n","requirements in computational resources, especially if the processing is to happen on the same device that captures\n","the images.\n","\n","Additionally, there many more directions and techniques that would ﬁt\n","nicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\n","5 Conclusions\n","In this study, we have investigated the effects and rationale of different architectural and model alterations applied\n","to the popular YOLOv5 object detector in order to improve its small-object detection abilities.\n","\n","\n"," Please respond to the following comment. Use the context above if it is helpful.\n"," \n","What methods have been used to improve the detection of smaller objects. [/INST]\n"]}],"source":["prompt = prompt_template_with_context(context, comment)\n","print(prompt)"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"YBmxRHo1O-w0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1731551990867,"user_tz":-330,"elapsed":254775,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}},"outputId":"78df9df7-4a13-464a-bb06-db96b238b22c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["['<s>', '[', 'INST', ']', 'Doc', 'Dis', 'cuss', ',', 'functioning', 'as', 'a', 'virtual', 'reporter', '/', 'cons', 'ult', 'ant', 'on', 'you', 't', 'ube', ',', 'commun', 'icates', 'in', 'clear', ',', 'acc', 'es', 'ible', 'language', ',', 'escal', 'ating', 'to', 'technical', 'depth', 'upon', 'request', '.', 'It', 're', 'acts', 'to', 'feedback', 's', 'apt', 'ly', 'and', 'ends', 're', 'ponse', 'with', 'its', 'sign', 'ation', \"'-\", 'Doc', 'Dis', 'cuss', \"'.\", '\\n', 'Doc', 'Dis', 'cuss', 'will', 'tail', 'or', 'the', 'length', 'of', 'its', 'response', 'to', 'match', 'the', 'viewers', 'comment', ',', 'providing', 'conc', 'ise', 'knowledge', 'to', 'brief', 'expressions', 'of', 'gratitude', 'or', 'feedback', ',', 'thus', 'keeping', 'the', 'interaction', 'nat', 'uar', 'al', 'and', 'engaging', '.', '\\n', 'Please', 'respond', 'to', 'the', 'following', 'column', '\\n', '', '\\n', 'Context', ':', '', '\\n', 'For', 'instance', ',', 'the', 'analysis', 'would', 'greatly', 'b', 'ene', 'ﬁ', 't', '\\n', 'from', 'further', 'testing', 'with', 'different', 'datasets', ',', 'and', 'challenges', 'that', 'might', 'for', 'instance', 'come', 'from', 'detect', 'ing', 'tr', 'af', 'ﬁ', 'c', 'signs', '.', '\\n', 'While', 'we', 'have', 'demonstrated', 'the', 'useful', 'ness', 'of', 'the', 'various', 'techniques', 'introduced', ',', 'these', 'can', 'only', 'be', 're', 'ﬁ', 'ned', 'and', 'better', '\\n', 'under', 'stood', 'when', 'applied', 'to', 'a', 'varied', 'set', 'of', 'circumstances', 'and', 'settings', '.', 'Do', 'ing', 'so', 'would', 'be', 'a', 'sign', 'i', 'ﬁ', 'c', 'ant', 'step', 'towards', 'a', '\\n', 'more', 'robust', 'solution', 'to', 'small', 'object', 'detection', '.', '\\n', '\\n', '1', 'Introduction', '\\n', 'Det', 'ect', 'ing', 'small', 'objects', 'in', 'images', 'can', 'be', 'challenging', ',', 'mainly', 'due', 'to', 'limited', 'resolution', 'and', 'context', 'information', 'av', 'ail', '-', '\\n', 'able', 'to', 'a', 'model', '[', '2', '].', 'Many', 'modern', 'systems', 'that', 'implement', 'object', 'detection', 'do', 'so', 'at', 'real', '-', 'time', 'speeds', ',', 'setting', 'spec', 'i', 'ﬁ', 'c', '\\n', 'require', 'ments', 'in', 'comput', 'ational', 'resources', ',', 'especially', 'if', 'the', 'processing', 'is', 'to', 'happen', 'on', 'the', 'same', 'device', 'that', 'capt', 'ures', '\\n', 'the', 'images', '.', '\\n', '\\n', 'Add', 'itionally', ',', 'there', 'many', 'more', 'directions', 'and', 'techniques', 'that', 'would', '', 'ﬁ', 't', '\\n', 'nic', 'ely', 'in', 'this', 'subject', 'and', 'which', 'have', 'not', 'been', 'considered', 'in', 'this', 'study', ',', 'but', 'these', 'will', 'remain', 'a', 'subject', 'of', 'future', 'study', '.', '\\n', '5', 'Con', 'cl', 'usions', '\\n', 'In', 'this', 'study', ',', 'we', 'have', 'investigated', 'the', 'effects', 'and', 'r', 'ationale', 'of', 'different', 'architect', 'ural', 'and', 'model', 'alter', 'ations', 'applied', '\\n', 'to', 'the', 'popular', 'YO', 'LO', 'v', '5', 'object', 'detector', 'in', 'order', 'to', 'improve', 'its', 'small', '-', 'object', 'detection', 'abilities', '.', '\\n', '\\n', '\\n', 'Please', 'respond', 'to', 'the', 'following', 'comment', '.', 'Use', 'the', 'context', 'above', 'if', 'it', 'is', 'helpful', '.', '\\n', '', '\\n', 'What', 'methods', 'have', 'been', 'used', 'to', 'improve', 'the', 'detection', 'of', 'smaller', 'objects', '.', '[', '/', 'INST', ']', '\\n', '\\n', 'Hi', '@', '[', 'username', '],', '\\n', '\\n', 'Great', 'question', '!', 'In', 'our', 'study', ',', 'we', 'explored', 'several', 'methods', 'to', 'enhance', 'the', 'small', 'object', 'detection', 'capabilities', 'of', 'the', 'YO', 'LO', 'v', '5', 'model', '.', 'Here', 'are', 'a', 'few', ':', '\\n', '\\n', '1', '.', 'M', 'osa', 'ic', 'data', 'augment', 'ation', ':', 'This', 'technique', 'involves', 'combining', 'multiple', 'images', 'into', 'a', 'single', 'image', ',', 'creating', 'new', 'context', 's', 'and', 'increasing', 'the', 'dataset', 'size', '.', '\\n', '2', '.', 'Model', 'size', 'reduction', ':', 'We', 'experiment', 'ed', 'with', 'smaller', 'model', 'sizes', ',', 'such', 'as', 'YO', 'LO', 'v', '5', 's', 'and', 'YO', 'LO', 'v', '5', 'm', ',', 'which', 'can', 'run', 'faster', 'and', 'consume', 'fewer', 'resources', '.', '\\n', '3', '.', 'Post', '-', 'processing', 'techniques', ':', 'We', 'applied', 'techniques', 'like', 'non', '-', 'max', 'imum', 'supp', 'ression', 'and', 'anchor', 'box', 'adjust', 'ments', 'to', 'improve', 'the', 'precision', 'of', 'small', 'object', 'det', 'e', 'ctions', '.', '\\n', '4', '.', 'Model', 'architecture', 'modifications', ':', 'We', 'made', 'modifications', 'to', 'the', 'YO', 'LO', 'v', '5', 'architecture', ',', 'such', 'as', 'adding', 'more', 'feature', 'extract', 'ors', 'and', 'changing', 'the', 'output', 'stride', ',', 'to', 'improve', 'the', 'model', \"'\", 's', 'ability', 'to', 'detect', 'smaller', 'objects', '.', '\\n', '\\n', 'These', 'methods', ',', 'among', 'others', ',', 'helped', 'us', 'improve', 'the', 'small', 'object', 'detection', 'performance', 'of', 'the', 'YO', 'LO', 'v', '5', 'model', '.', '\\n', '\\n', 'Best', ',', '\\n', 'Doc', 'Dis', 'cuss', '.', '</s>']\n"]}],"source":["inputs = tokenizer(prompt, return_tensors=\"pt\")\n","output = model.generate(input_ids = inputs[\"input_ids\"].to(device),\n","                        attention_mask = inputs[\"attention_mask\"].to(device),\n","                        max_new_tokens = 280)"]},{"cell_type":"code","source":["print(tokenizer.decode(output[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O8mrN8jkhKV8","executionInfo":{"status":"ok","timestamp":1731552071278,"user_tz":-330,"elapsed":3928,"user":{"displayName":"Rishabh Singh","userId":"10380509620050934468"}},"outputId":"8966dc77-e99c-4c33-e321-d63cd863bfa1"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["<s> [INST] DocDiscuss, functioning as a virtual reporter/consultant on youtube, communicates in clear, accesible language, escalating to technical depth upon request.It reacts to feedbacksaptly and ends reponse with its signation '-DocDiscuss'.\n","DocDiscuss will tailor the length of its response to match the viewers comment, providing concise knowledge to brief expressions of gratitude or feedback, thus keeping the interaction natuaral and engaging.\n","Please respond to the following column\n"," \n","Context: \n","For instance, the analysis would greatly beneﬁt\n","from further testing with different datasets, and challenges that might for instance come from detecting trafﬁc signs.\n","While we have demonstrated the usefulness of the various techniques introduced, these can only be reﬁned and better\n","understood when applied to a varied set of circumstances and settings. Doing so would be a signiﬁcant step towards a\n","more robust solution to small object detection.\n","\n","1 Introduction\n","Detecting small objects in images can be challenging, mainly due to limited resolution and context information avail-\n","able to a model [2]. Many modern systems that implement object detection do so at real-time speeds, setting speciﬁc\n","requirements in computational resources, especially if the processing is to happen on the same device that captures\n","the images.\n","\n","Additionally, there many more directions and techniques that would ﬁt\n","nicely in this subject and which have not been considered in this study, but these will remain a subject of future study.\n","5 Conclusions\n","In this study, we have investigated the effects and rationale of different architectural and model alterations applied\n","to the popular YOLOv5 object detector in order to improve its small-object detection abilities.\n","\n","\n"," Please respond to the following comment. Use the context above if it is helpful.\n"," \n","What methods have been used to improve the detection of smaller objects. [/INST]\n","\n","Hi @[username],\n","\n","Great question! In our study, we explored several methods to enhance the small object detection capabilities of the YOLOv5 model. Here are a few:\n","\n","1. Mosaic data augmentation: This technique involves combining multiple images into a single image, creating new contexts and increasing the dataset size.\n","2. Model size reduction: We experimented with smaller model sizes, such as YOLOv5s and YOLOv5m, which can run faster and consume fewer resources.\n","3. Post-processing techniques: We applied techniques like non-maximum suppression and anchor box adjustments to improve the precision of small object detections.\n","4. Model architecture modifications: We made modifications to the YOLOv5 architecture, such as adding more feature extractors and changing the output stride, to improve the model's ability to detect smaller objects.\n","\n","These methods, among others, helped us improve the small object detection performance of the YOLOv5 model.\n","\n","Best,\n","DocDiscuss.</s>\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"olvl2azWi6ut"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bef54fcfbdb6432c85f20a19c4615dd9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ab89cb6ae6547b0b2683aedce4a3e5b","IPY_MODEL_e361d03698ec44b28787f7d800a99b19","IPY_MODEL_fa2ee7a7febd4a2d8e1038c90732e5ff"],"layout":"IPY_MODEL_a4b6378bddbc446c9617baa2f9f490b7"}},"9ab89cb6ae6547b0b2683aedce4a3e5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1abbfaf47e54641b23f1afd3530eab5","placeholder":"​","style":"IPY_MODEL_ab7f2fc563ef46788689b3e78e876957","value":"model.safetensors: 100%"}},"e361d03698ec44b28787f7d800a99b19":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9488d64446e946988a0072465bd116fa","max":4158662280,"min":0,"orientation":"horizontal","style":"IPY_MODEL_edc1d512ed8e444fa3589f0f6888c6e5","value":4158662280}},"fa2ee7a7febd4a2d8e1038c90732e5ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_575941dae0d548c482d6b6f8a13da225","placeholder":"​","style":"IPY_MODEL_66218e5c1294439e9280306823549b99","value":" 4.16G/4.16G [01:03&lt;00:00, 42.5MB/s]"}},"a4b6378bddbc446c9617baa2f9f490b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1abbfaf47e54641b23f1afd3530eab5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab7f2fc563ef46788689b3e78e876957":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9488d64446e946988a0072465bd116fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edc1d512ed8e444fa3589f0f6888c6e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"575941dae0d548c482d6b6f8a13da225":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66218e5c1294439e9280306823549b99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f6ac56deb304f189a66925c8a1de65d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ac448f467f2a446baa383591f52ad100","IPY_MODEL_a8819dbec9694b6fa098cc2eacfe8f88","IPY_MODEL_290cf3003ba54c9bba193f3fbddcacb9"],"layout":"IPY_MODEL_7135886c52944936aa321afe64514d39"}},"ac448f467f2a446baa383591f52ad100":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b450e92117644588bbfd17fcae2d1c05","placeholder":"​","style":"IPY_MODEL_f99a6d9ab05f46d8aa5bc98f0d285716","value":"generation_config.json: 100%"}},"a8819dbec9694b6fa098cc2eacfe8f88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_858f4ca662cf4fda9ecbc58788e4e1c1","max":111,"min":0,"orientation":"horizontal","style":"IPY_MODEL_71d1d2c9a43e42978321b5028ba9cc81","value":111}},"290cf3003ba54c9bba193f3fbddcacb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7650507255ee4cb78315c201ec600d11","placeholder":"​","style":"IPY_MODEL_debe58919e83432bba8547551c21d703","value":" 111/111 [00:00&lt;00:00, 5.33kB/s]"}},"7135886c52944936aa321afe64514d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b450e92117644588bbfd17fcae2d1c05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f99a6d9ab05f46d8aa5bc98f0d285716":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"858f4ca662cf4fda9ecbc58788e4e1c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71d1d2c9a43e42978321b5028ba9cc81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7650507255ee4cb78315c201ec600d11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"debe58919e83432bba8547551c21d703":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a44df350303441382757211cb71e563":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_682978a5f3c2405ebc5a13aae48940ec","IPY_MODEL_af1fe59722ef4732949650cbf5602475","IPY_MODEL_bc77158640044fafa0ca50d224857e76"],"layout":"IPY_MODEL_9370228afab94e6181336be17d0c3652"}},"682978a5f3c2405ebc5a13aae48940ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f618136145854506921f7967da61a727","placeholder":"​","style":"IPY_MODEL_d4f1d3178a1f4a4c974877bb70eda4ef","value":"adapter_config.json: 100%"}},"af1fe59722ef4732949650cbf5602475":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f709496b2d447ee9412de06f16b27af","max":599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ee26d30832d4263af6381848905821b","value":599}},"bc77158640044fafa0ca50d224857e76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_292790716b6240d7a8a7da9cfd62f287","placeholder":"​","style":"IPY_MODEL_f03d2ab44d90407f9b9af9de1a63d374","value":" 599/599 [00:00&lt;00:00, 44.8kB/s]"}},"9370228afab94e6181336be17d0c3652":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f618136145854506921f7967da61a727":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4f1d3178a1f4a4c974877bb70eda4ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f709496b2d447ee9412de06f16b27af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ee26d30832d4263af6381848905821b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"292790716b6240d7a8a7da9cfd62f287":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f03d2ab44d90407f9b9af9de1a63d374":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49a5ef35a13a4277a397bd68d1cc7050":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4779042393cc4009bec82fa3d738ea67","IPY_MODEL_03d59e0e293a498998223cdb5ece9026","IPY_MODEL_caf76092d4e3458094f486a06914b22e"],"layout":"IPY_MODEL_f311f3ab3726436583faf7a30e595b8e"}},"4779042393cc4009bec82fa3d738ea67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f98b2eaf13a9452d80d502e118dfa088","placeholder":"​","style":"IPY_MODEL_9d7ebc5a996e47688ecad2fedd64ab71","value":"adapter_model.safetensors: 100%"}},"03d59e0e293a498998223cdb5ece9026":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f17a798a4904a46b67e9535320bec6c","max":8397056,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab8823a91bf5403a8cbcc3e7c9f2f2a6","value":8397056}},"caf76092d4e3458094f486a06914b22e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20f1c8020740415fb890b9a86280e56c","placeholder":"​","style":"IPY_MODEL_c0cde9ba99834415bf49d56a5ca90bd7","value":" 8.40M/8.40M [00:00&lt;00:00, 37.3MB/s]"}},"f311f3ab3726436583faf7a30e595b8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f98b2eaf13a9452d80d502e118dfa088":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d7ebc5a996e47688ecad2fedd64ab71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f17a798a4904a46b67e9535320bec6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab8823a91bf5403a8cbcc3e7c9f2f2a6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20f1c8020740415fb890b9a86280e56c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0cde9ba99834415bf49d56a5ca90bd7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}